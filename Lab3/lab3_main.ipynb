{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing the Data for DNN Training \n",
    "\n",
    "## 4.1 Target Class Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ah_0', 'ah_1', 'ah_2', 'ao_0', 'ao_1', 'ao_2', 'ay_0', 'ay_1', 'ay_2', 'eh_0', 'eh_1', 'eh_2', 'ey_0', 'ey_1', 'ey_2', 'f_0', 'f_1', 'f_2', 'ih_0', 'ih_1', 'ih_2', 'iy_0', 'iy_1', 'iy_2', 'k_0', 'k_1', 'k_2', 'n_0', 'n_1', 'n_2', 'ow_0', 'ow_1', 'ow_2', 'r_0', 'r_1', 'r_2', 's_0', 's_1', 's_2', 'sil_0', 'sil_1', 'sil_2', 'sp_0', 't_0', 't_1', 't_2', 'th_0', 'th_1', 'th_2', 'uw_0', 'uw_1', 'uw_2', 'v_0', 'v_1', 'v_2', 'w_0', 'w_1', 'w_2', 'z_0', 'z_1', 'z_2']\n",
      "\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "phoneHMMs = np.load(\"../Lab2/lab2_models_all.npz\", allow_pickle=True)[\"phoneHMMs\"].item()\n",
    "phones = sorted(phoneHMMs.keys())\n",
    "nstates = {phone: phoneHMMs[phone]['means'].shape[0] for phone in phones}\n",
    "stateList = [ph + '_' + str(id) for ph in phones for id in range(nstates[ph])]\n",
    "print(stateList)\n",
    "print()\n",
    "print(stateList.index('ay_2'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Forced Alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = '/content/drive/MyDrive/KTH/DD2119_Speech_Recognition/data/tidigits/tidigits/disc_4.1.1/tidigits/train/man/nw/z43a.wav'\n",
    "samples, samplingrate = loadAudio(filename)\n",
    "lmfcc = mfcc(samples)\n",
    "\n",
    "wordTrans = list(path2info(filename)[2])  # Transcription using words\n",
    "print(f\"wordTrans: {wordTrans}\")\n",
    "\n",
    "phoneTrans = words2phones(wordTrans, prondict) # Transcription using phonemes\n",
    "print(f\"phoneTrans: {phoneTrans}\")\n",
    "\n",
    "utteranceHMM = concatHMMs(phoneHMMs, phoneTrans)\n",
    "stateTrans = [phone + '_' + str(stateid) for phone in phoneTrans for stateid in range(nstates[phone])]  # Transcription using states\n",
    "print(f\"stateTrans[10]: {stateTrans[10]}\")\n",
    "\n",
    "obsloglik = log_multivariate_normal_density_diag(lmfcc, utteranceHMM[\"means\"], utteranceHMM[\"covars\"])\n",
    "viterbiLoglik, viterbiPath = viterbi(obsloglik, np.log(utteranceHMM['startprob'][:-1]), np.log(utteranceHMM['transmat'][:-1, :-1]), forceFinalState=True)\n",
    "\n",
    "viterbiStateTrans = [stateTrans[state] for state in viterbiPath]\n",
    "\n",
    "trans = frames2trans(viterbiStateTrans, outfilename='z43a.lab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mspec_res = mspec(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = np.load(\"lab3_example.npz\", allow_pickle=True)[\"example\"].item()\n",
    "# Compare each variable with its corresponding value in the example dictionary\n",
    "\n",
    "print(f\"lmfcc: {np.allclose(lmfcc, example['lmfcc'])}\")\n",
    "print(f\"Our wordTrans: \\n{wordTrans}\\nCorrect wordTrans: \\n{example['wordTrans']}\")\n",
    "print(f\"Our phoneTrans: \\n{phoneTrans}\\nCorrect phoneTrans: \\n{example['phoneTrans']}\")\n",
    "print(f\"Our stateTrans: \\n{stateTrans}\\nCorrect stateTrans: \\n{example['stateTrans']}\")\n",
    "print(f\"obsloglik: {np.allclose(obsloglik, example['obsloglik'])}\")\n",
    "print(f\"viterbiLoglik: {np.allclose(viterbiLoglik, example['viterbiLoglik'])}\")\n",
    "print(f\"viterbiPath: {np.allclose(viterbiPath, example['viterbiPath'])}\")\n",
    "print(f\"Our viterbiStateTrans: \\n{viterbiStateTrans}\\nCorrect viterbiStateTrans: \\n{example['viterbiStateTrans']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Feature Extraction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "def feature_extraction(path):\n",
    "  data = []\n",
    "\n",
    "  for root, dirs, files in os.walk(path):\n",
    "    for file in tqdm(files):\n",
    "      if file.endswith('.wav'):\n",
    "        filename = os.path.join(root, file)\n",
    "        samples, samplingrate = loadAudio(filename)\n",
    "\n",
    "        lmfcc = mfcc(samples) # Features used for HMM & DNN\n",
    "        mspec_res = mspec(samples) # Features used for DNN\n",
    "\n",
    "        wordTrans = list(path2info(filename))[2]  # Transcription using words\n",
    "        phoneTrans = words2phones(wordTrans, prondict) # Transcription using phonemes\n",
    "        targets = forcedAlignment(lmfcc, phoneHMMs, phoneTrans) # Align states to each utterance\n",
    "\n",
    "        # converting targets to indices to save memory\n",
    "        target_idx = np.array([stateList.index(target) for target in targets])\n",
    "\n",
    "        data.append({'filename': filename, 'lmfcc': lmfcc,'mspec': mspec_res, 'targets': target_idx})\n",
    "\n",
    "  return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(\"Extraction features from train data\")\n",
    "#trainData = feature_extraction('/content/drive/MyDrive/KTH/DD2119_Speech_Recognition/Labs/data/tidigits/tidigits/disc_4.1.1/tidigits/train')\n",
    "# Save the data to avoid computing it again\n",
    "#np.savez('trainData.npz', trainData=trainData)\n",
    "\n",
    "print(\"Extracting features from test data\")\n",
    "testData = feature_extraction('/content/drive/MyDrive/KTH/DD2119_Speech_Recognition/Labs/data/tidigits/tidigits/disc_4.2.1/tidigits/test')\n",
    "np.savez('testData.npz', testData=testData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "\n",
    "# Specify the path to the files in your Google Drive\n",
    "train_data_path = '/content/trainData.npz'\n",
    "test_data_path = '/content/testData.npz'\n",
    "\n",
    "\n",
    "# Download the files\n",
    "# files.download(train_data_path)\n",
    "files.download(test_data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 Training and Validation Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitDataByGender(data_dict,gender, train_utterances):\n",
    "  train_data, val_data = [], []\n",
    "  for current_speaker in data_dict[gender].keys():\n",
    "    # if train_data contains 90 or more procent of total gender utterances, remaining data is stored as validation\n",
    "    if len(train_data) >= train_utterances:\n",
    "      #print(f\"len(train_data): {len(train_data)} > {train_utterances} --> Creating val set instead\")\n",
    "      val_data.extend(data_dict[gender][str(current_speaker)])\n",
    "    # Otherwise, we keep adding to train data until we achieve 90%\n",
    "    else:\n",
    "      #print(f\"len(train_data): {len(train_data)} < {train_utterances}\")\n",
    "      train_data.extend(data_dict[gender][str(current_speaker)])\n",
    "  print(f\"train_data: {len(train_data)} \\t val_data: {len(val_data)}\")\n",
    "  return train_data, val_data\n",
    "\n",
    "def splitData(total_data, split=0.1):\n",
    "  data_by_gender = {\"man\":{}, \"woman\": {}}\n",
    "  print(f\"Total_data length = {len(total_data)}\")\n",
    "  for data in total_data:\n",
    "    gender, speakerID, _, _ = path2info(data[\"filename\"])  # path2info returns tuple (gender, speakerID, digits, repetition)\n",
    "    if speakerID not in data_by_gender[gender]:\n",
    "      data_by_gender[gender][speakerID] = []\n",
    "    data_by_gender[gender][speakerID].append(data)\n",
    "\n",
    "  # Calculate total utterances by summing the lengths of each gender's list\n",
    "  total_male_utterances = sum(len(utterances) for utterances in data_by_gender[\"man\"].values())\n",
    "  total_female_utterances = sum(len(utterances) for utterances in data_by_gender[\"woman\"].values())\n",
    "\n",
    "  train_male_utterances = int(total_male_utterances * (1-split))     # compute how many male utterances to achieve 90%\n",
    "  train_female_utterances = int(total_female_utterances * (1-split)) # compute how many female utterances to achieve 90%\n",
    "  print(f\"total male utterances: {total_male_utterances}\\ntrain_male_utterances: {train_male_utterances}\")\n",
    "  print(f\"total female utterances: {total_female_utterances}\\ntrain_female_utterances: {train_female_utterances}\")\n",
    "\n",
    "  male_train_data, male_val_data = splitDataByGender(data_by_gender, \"man\", train_male_utterances)\n",
    "  female_train_data, female_val_data = splitDataByGender(data_by_gender, \"woman\", train_female_utterances)\n",
    "\n",
    "  train_data, val_data = [], []\n",
    "  train_data.extend(male_train_data)\n",
    "  train_data.extend(female_train_data)\n",
    "  val_data.extend(male_val_data)\n",
    "  val_data.extend(female_val_data)\n",
    "\n",
    "  print(f\"train data has {len(train_data)} elements\")\n",
    "  print(f\"val data has {len(val_data)} elements\")\n",
    "\n",
    "  return train_data, val_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainData, valData = splitData(trainData)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5 Acoustic Context (Dynamic Features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.6 Feature Standardisation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 Phoneme Recognition with Deep Neural Networks\n",
    "\n",
    "## 5.1 Detailed Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Possible Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
