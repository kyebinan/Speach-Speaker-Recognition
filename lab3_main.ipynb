{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import soundfile as sf\n",
    "import os \n",
    "from tqdm import tqdm\n",
    "\n",
    "from Lab3 import lab3_proto as proto3\n",
    "from Lab3 import lab3_tools as tools3\n",
    "from Lab2 import lab2_proto as proto2\n",
    "from Lab2 import lab2_tools as tools2\n",
    "from Lab1 import lab1_proto as proto1\n",
    "from Lab1 import lab1_tools as tools1\n",
    "from Lab2.prondict import prondict "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing the Data for DNN Training \n",
    "\n",
    "## 4.1 Target Class Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ah_0', 'ah_1', 'ah_2', 'ao_0', 'ao_1', 'ao_2', 'ay_0', 'ay_1', 'ay_2', 'eh_0', 'eh_1', 'eh_2', 'ey_0', 'ey_1', 'ey_2', 'f_0', 'f_1', 'f_2', 'ih_0', 'ih_1', 'ih_2', 'iy_0', 'iy_1', 'iy_2', 'k_0', 'k_1', 'k_2', 'n_0', 'n_1', 'n_2', 'ow_0', 'ow_1', 'ow_2', 'r_0', 'r_1', 'r_2', 's_0', 's_1', 's_2', 'sil_0', 'sil_1', 'sil_2', 'sp_0', 't_0', 't_1', 't_2', 'th_0', 'th_1', 'th_2', 'uw_0', 'uw_1', 'uw_2', 'v_0', 'v_1', 'v_2', 'w_0', 'w_1', 'w_2', 'z_0', 'z_1', 'z_2']\n",
      "\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "phoneHMMs = np.load(\"./Lab2/lab2_models_all.npz\", allow_pickle=True)[\"phoneHMMs\"].item()\n",
    "phones = sorted(phoneHMMs.keys())\n",
    "nstates = {phone: phoneHMMs[phone]['means'].shape[0] for phone in phones}\n",
    "stateList = [ph + '_' + str(id) for ph in phones for id in range(nstates[ph])]\n",
    "print(stateList)\n",
    "print()\n",
    "print(stateList.index('ay_2'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Forced Alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wordTrans: ['z', '4', '3']\n",
      "phoneTrans: ['sil', 'z', 'iy', 'r', 'ow', 'sp', 'f', 'ao', 'r', 'sp', 'th', 'r', 'iy', 'sp', 'sil']\n",
      "stateTrans[10]: r_1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_763028/2984385968.py:17: RuntimeWarning: divide by zero encountered in log\n",
      "  viterbiLoglik, viterbiPath = proto2.viterbi(obsloglik, np.log(utteranceHMM['startprob'][:-1]), np.log(utteranceHMM['transmat'][:-1, :-1]), forceFinalState=True)\n"
     ]
    }
   ],
   "source": [
    "# the dataset is not on the github due to the copyright\n",
    "filename = './../tidigits/disc_4.1.1/tidigits/train/man/nw/z43a.wav'\n",
    "samples, samplingrate = tools3.loadAudio(filename)\n",
    "lmfcc = proto1.mfcc(samples)\n",
    "\n",
    "wordTrans = list(tools3.path2info(filename)[2])  # Transcription using words\n",
    "print(f\"wordTrans: {wordTrans}\")\n",
    "\n",
    "phoneTrans = proto3.words2phones(wordTrans, prondict) # Transcription using phonemes\n",
    "print(f\"phoneTrans: {phoneTrans}\")\n",
    "\n",
    "utteranceHMM = proto2.concatHMMs(phoneHMMs, phoneTrans)\n",
    "stateTrans = [phone + '_' + str(stateid) for phone in phoneTrans for stateid in range(nstates[phone])]  # Transcription using states\n",
    "print(f\"stateTrans[10]: {stateTrans[10]}\")\n",
    "\n",
    "obsloglik = tools2.log_multivariate_normal_density_diag(lmfcc, utteranceHMM[\"means\"], utteranceHMM[\"covars\"])\n",
    "viterbiLoglik, viterbiPath = proto2.viterbi(obsloglik, np.log(utteranceHMM['startprob'][:-1]), np.log(utteranceHMM['transmat'][:-1, :-1]), forceFinalState=True)\n",
    "\n",
    "viterbiStateTrans = [stateTrans[state] for state in viterbiPath]\n",
    "\n",
    "trans = tools3.frames2trans(viterbiStateTrans, outfilename='z43a.lab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mspec_res = proto1.mspec(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lmfcc: True\n",
      "Our wordTrans: \n",
      "['z', '4', '3']\n",
      "Correct wordTrans: \n",
      "['z', '4', '3']\n",
      "Our phoneTrans: \n",
      "['sil', 'z', 'iy', 'r', 'ow', 'sp', 'f', 'ao', 'r', 'sp', 'th', 'r', 'iy', 'sp', 'sil']\n",
      "Correct phoneTrans: \n",
      "['sil', 'z', 'iy', 'r', 'ow', 'sp', 'f', 'ao', 'r', 'sp', 'th', 'r', 'iy', 'sp', 'sil']\n",
      "Our stateTrans: \n",
      "['sil_0', 'sil_1', 'sil_2', 'z_0', 'z_1', 'z_2', 'iy_0', 'iy_1', 'iy_2', 'r_0', 'r_1', 'r_2', 'ow_0', 'ow_1', 'ow_2', 'sp_0', 'f_0', 'f_1', 'f_2', 'ao_0', 'ao_1', 'ao_2', 'r_0', 'r_1', 'r_2', 'sp_0', 'th_0', 'th_1', 'th_2', 'r_0', 'r_1', 'r_2', 'iy_0', 'iy_1', 'iy_2', 'sp_0', 'sil_0', 'sil_1', 'sil_2']\n",
      "Correct stateTrans: \n",
      "['sil_0', 'sil_1', 'sil_2', 'z_0', 'z_1', 'z_2', 'iy_0', 'iy_1', 'iy_2', 'r_0', 'r_1', 'r_2', 'ow_0', 'ow_1', 'ow_2', 'sp_0', 'f_0', 'f_1', 'f_2', 'ao_0', 'ao_1', 'ao_2', 'r_0', 'r_1', 'r_2', 'sp_0', 'th_0', 'th_1', 'th_2', 'r_0', 'r_1', 'r_2', 'iy_0', 'iy_1', 'iy_2', 'sp_0', 'sil_0', 'sil_1', 'sil_2']\n",
      "obsloglik: True\n",
      "viterbiLoglik: True\n",
      "viterbiPath: True\n",
      "Our viterbiStateTrans: \n",
      "['sil_0', 'sil_1', 'sil_1', 'sil_1', 'sil_1', 'sil_1', 'sil_1', 'sil_1', 'sil_1', 'sil_1', 'sil_1', 'sil_1', 'sil_1', 'sil_1', 'sil_1', 'sil_1', 'sil_1', 'sil_1', 'sil_1', 'sil_2', 'z_0', 'z_0', 'z_0', 'z_0', 'z_1', 'z_2', 'z_2', 'z_2', 'z_2', 'z_2', 'z_2', 'z_2', 'z_2', 'z_2', 'z_2', 'z_2', 'iy_0', 'iy_0', 'iy_0', 'iy_0', 'iy_0', 'iy_0', 'iy_0', 'iy_0', 'iy_1', 'iy_2', 'r_0', 'r_0', 'r_0', 'r_0', 'r_0', 'r_0', 'r_0', 'r_0', 'r_0', 'r_0', 'r_1', 'r_2', 'ow_0', 'ow_1', 'ow_2', 'ow_2', 'ow_2', 'ow_2', 'ow_2', 'ow_2', 'ow_2', 'ow_2', 'ow_2', 'f_0', 'f_1', 'f_1', 'f_1', 'f_1', 'f_1', 'f_1', 'f_1', 'f_1', 'f_1', 'f_1', 'f_1', 'f_2', 'ao_0', 'ao_1', 'ao_1', 'ao_1', 'ao_1', 'ao_1', 'ao_1', 'ao_1', 'ao_1', 'ao_1', 'ao_1', 'ao_1', 'ao_1', 'ao_1', 'ao_1', 'ao_2', 'ao_2', 'ao_2', 'ao_2', 'ao_2', 'ao_2', 'ao_2', 'ao_2', 'ao_2', 'ao_2', 'ao_2', 'r_0', 'r_0', 'r_0', 'r_1', 'r_2', 'th_0', 'th_0', 'th_0', 'th_0', 'th_0', 'th_0', 'th_0', 'th_0', 'th_0', 'th_0', 'th_1', 'th_1', 'th_1', 'th_2', 'r_0', 'r_0', 'r_0', 'r_0', 'r_0', 'r_0', 'r_0', 'r_0', 'r_0', 'r_1', 'r_2', 'iy_0', 'iy_0', 'iy_0', 'iy_0', 'iy_0', 'iy_0', 'iy_0', 'iy_0', 'iy_0', 'iy_0', 'iy_1', 'iy_1', 'iy_2', 'iy_2', 'iy_2', 'iy_2', 'iy_2', 'iy_2', 'iy_2', 'iy_2', 'sil_0', 'sil_0', 'sil_0', 'sil_0', 'sil_0', 'sil_0', 'sil_0', 'sil_0', 'sil_0', 'sil_0', 'sil_0', 'sil_0', 'sil_0', 'sil_0', 'sil_0', 'sil_0', 'sil_0', 'sil_0', 'sil_1', 'sil_2']\n",
      "Correct viterbiStateTrans: \n",
      "['sil_0', 'sil_1', 'sil_1', 'sil_1', 'sil_1', 'sil_1', 'sil_1', 'sil_1', 'sil_1', 'sil_1', 'sil_1', 'sil_1', 'sil_1', 'sil_1', 'sil_1', 'sil_1', 'sil_1', 'sil_1', 'sil_1', 'sil_2', 'z_0', 'z_0', 'z_0', 'z_0', 'z_1', 'z_2', 'z_2', 'z_2', 'z_2', 'z_2', 'z_2', 'z_2', 'z_2', 'z_2', 'z_2', 'z_2', 'iy_0', 'iy_0', 'iy_0', 'iy_0', 'iy_0', 'iy_0', 'iy_0', 'iy_0', 'iy_1', 'iy_2', 'r_0', 'r_0', 'r_0', 'r_0', 'r_0', 'r_0', 'r_0', 'r_0', 'r_0', 'r_0', 'r_1', 'r_2', 'ow_0', 'ow_1', 'ow_2', 'ow_2', 'ow_2', 'ow_2', 'ow_2', 'ow_2', 'ow_2', 'ow_2', 'ow_2', 'f_0', 'f_1', 'f_1', 'f_1', 'f_1', 'f_1', 'f_1', 'f_1', 'f_1', 'f_1', 'f_1', 'f_1', 'f_2', 'ao_0', 'ao_1', 'ao_1', 'ao_1', 'ao_1', 'ao_1', 'ao_1', 'ao_1', 'ao_1', 'ao_1', 'ao_1', 'ao_1', 'ao_1', 'ao_1', 'ao_1', 'ao_2', 'ao_2', 'ao_2', 'ao_2', 'ao_2', 'ao_2', 'ao_2', 'ao_2', 'ao_2', 'ao_2', 'ao_2', 'r_0', 'r_0', 'r_0', 'r_1', 'r_2', 'th_0', 'th_0', 'th_0', 'th_0', 'th_0', 'th_0', 'th_0', 'th_0', 'th_0', 'th_0', 'th_1', 'th_1', 'th_1', 'th_2', 'r_0', 'r_0', 'r_0', 'r_0', 'r_0', 'r_0', 'r_0', 'r_0', 'r_0', 'r_1', 'r_2', 'iy_0', 'iy_0', 'iy_0', 'iy_0', 'iy_0', 'iy_0', 'iy_0', 'iy_0', 'iy_0', 'iy_0', 'iy_1', 'iy_1', 'iy_2', 'iy_2', 'iy_2', 'iy_2', 'iy_2', 'iy_2', 'iy_2', 'iy_2', 'sil_0', 'sil_0', 'sil_0', 'sil_0', 'sil_0', 'sil_0', 'sil_0', 'sil_0', 'sil_0', 'sil_0', 'sil_0', 'sil_0', 'sil_0', 'sil_0', 'sil_0', 'sil_0', 'sil_0', 'sil_0', 'sil_1', 'sil_2']\n"
     ]
    }
   ],
   "source": [
    "example = np.load(\"./Lab3/lab3_example.npz\", allow_pickle=True)[\"example\"].item()\n",
    "# Compare each variable with its corresponding value in the example dictionary\n",
    "\n",
    "print(f\"lmfcc: {np.allclose(lmfcc, example['lmfcc'])}\")\n",
    "print(f\"Our wordTrans: \\n{wordTrans}\\nCorrect wordTrans: \\n{example['wordTrans']}\")\n",
    "print(f\"Our phoneTrans: \\n{phoneTrans}\\nCorrect phoneTrans: \\n{example['phoneTrans']}\")\n",
    "print(f\"Our stateTrans: \\n{stateTrans}\\nCorrect stateTrans: \\n{example['stateTrans']}\")\n",
    "print(f\"obsloglik: {np.allclose(obsloglik, example['obsloglik'])}\")\n",
    "print(f\"viterbiLoglik: {np.allclose(viterbiLoglik, example['viterbiLoglik'])}\")\n",
    "print(f\"viterbiPath: {np.allclose(viterbiPath, example['viterbiPath'])}\")\n",
    "print(f\"Our viterbiStateTrans: \\n{viterbiStateTrans}\\nCorrect viterbiStateTrans: \\n{example['viterbiStateTrans']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Feature Extraction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_extraction(path):\n",
    "  data = []\n",
    "\n",
    "  for root, dirs, files in os.walk(path):\n",
    "    for file in tqdm(files):\n",
    "      if file.endswith('.wav'):\n",
    "        filename = os.path.join(root, file)\n",
    "        samples, samplingrate = tools3.loadAudio(filename)\n",
    "\n",
    "        lmfcc = proto1.mfcc(samples) # Features used for HMM & DNN\n",
    "        mspec_res = proto1.mspec(samples) # Features used for DNN\n",
    "\n",
    "        wordTrans = list(tools3.path2info(filename))[2]  # Transcription using words\n",
    "        phoneTrans = proto3.words2phones(wordTrans, prondict) # Transcription using phonemes\n",
    "        targets = proto3.forcedAlignment(lmfcc, phoneHMMs, phoneTrans) # Align states to each utterance\n",
    "\n",
    "        # converting targets to indices to save memory\n",
    "        target_idx = np.array([stateList.index(target) for target in targets])\n",
    "\n",
    "        data.append({'filename': filename, 'lmfcc': lmfcc,'mspec': mspec_res, 'targets': target_idx})\n",
    "\n",
    "  return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting features from test data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "  0%|          | 0/77 [00:00<?, ?it/s]/home/yohan/Documents/Cantique_des_Cantiques/Project/Speach/Speach-Speaker-Recognition/Lab3/lab3_proto.py:49: RuntimeWarning: divide by zero encountered in log\n",
      "  _, viterbi_path = viterbi(obslogik, np.log(utteranceHMM[\"startprob\"][:-1]), np.log(utteranceHMM[\"transmat\"][:-1, :-1]), forceFinalState=True)\n",
      "100%|██████████| 77/77 [00:07<00:00, 10.30it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00,  9.64it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00, 10.31it/s]\n",
      "100%|██████████| 77/77 [00:06<00:00, 11.05it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00, 10.68it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00, 10.77it/s]\n",
      "100%|██████████| 77/77 [00:06<00:00, 11.38it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00, 10.01it/s]\n",
      "100%|██████████| 77/77 [00:06<00:00, 11.23it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00, 10.99it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00,  9.72it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00, 10.18it/s]\n",
      "100%|██████████| 77/77 [00:09<00:00,  8.38it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00,  9.85it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00,  9.77it/s]\n",
      "100%|██████████| 77/77 [00:08<00:00,  9.23it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00, 10.38it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00, 10.57it/s]\n",
      "100%|██████████| 77/77 [00:06<00:00, 11.58it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00, 10.14it/s]\n",
      "100%|██████████| 77/77 [00:08<00:00,  9.46it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00, 10.78it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00, 10.30it/s]\n",
      "100%|██████████| 77/77 [00:08<00:00,  9.43it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00, 10.17it/s]\n",
      "100%|██████████| 77/77 [00:08<00:00,  9.58it/s]\n",
      "100%|██████████| 77/77 [00:06<00:00, 11.23it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00, 10.77it/s]\n",
      "100%|██████████| 77/77 [00:06<00:00, 11.47it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00, 10.35it/s]\n",
      "100%|██████████| 77/77 [00:06<00:00, 11.42it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00, 10.53it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00, 10.87it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00,  9.97it/s]\n",
      "100%|██████████| 77/77 [00:08<00:00,  9.42it/s]\n",
      "100%|██████████| 77/77 [00:06<00:00, 11.63it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00, 10.62it/s]\n",
      "100%|██████████| 77/77 [00:06<00:00, 12.13it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00, 10.75it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00, 10.91it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00, 10.30it/s]\n",
      "100%|██████████| 77/77 [00:06<00:00, 11.49it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00, 10.03it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00, 10.54it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00, 10.23it/s]\n",
      "100%|██████████| 77/77 [00:06<00:00, 11.79it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00,  9.89it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00, 10.57it/s]\n",
      "100%|██████████| 77/77 [00:06<00:00, 11.41it/s]\n",
      "100%|██████████| 77/77 [00:06<00:00, 11.18it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00, 10.21it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00, 10.56it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00, 10.35it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00, 10.42it/s]\n",
      "100%|██████████| 77/77 [00:08<00:00,  8.95it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00, 10.91it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00, 10.27it/s]\n",
      "0it [00:00, ?it/s]\n",
      "100%|██████████| 77/77 [00:06<00:00, 11.09it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00,  9.80it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00,  9.99it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00, 10.72it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00, 10.31it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00, 10.92it/s]\n",
      "100%|██████████| 77/77 [00:06<00:00, 11.40it/s]\n",
      "100%|██████████| 77/77 [00:06<00:00, 11.45it/s]\n",
      "100%|██████████| 77/77 [00:06<00:00, 11.59it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00, 10.39it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00,  9.89it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00,  9.72it/s]\n",
      "100%|██████████| 77/77 [00:06<00:00, 11.59it/s]\n",
      "100%|██████████| 77/77 [00:06<00:00, 11.08it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00, 10.18it/s]\n",
      "100%|██████████| 77/77 [00:06<00:00, 12.57it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00,  9.79it/s]\n",
      "100%|██████████| 77/77 [00:09<00:00,  8.46it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00, 10.59it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00, 10.74it/s]\n",
      "100%|██████████| 77/77 [00:06<00:00, 11.48it/s]\n",
      "100%|██████████| 77/77 [00:06<00:00, 11.50it/s]\n",
      "100%|██████████| 77/77 [00:08<00:00,  9.00it/s]\n",
      "100%|██████████| 77/77 [00:06<00:00, 11.94it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00, 10.40it/s]\n",
      "100%|██████████| 76/76 [00:06<00:00, 11.34it/s]\n",
      "100%|██████████| 77/77 [00:06<00:00, 12.04it/s]\n",
      "100%|██████████| 77/77 [00:06<00:00, 11.46it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00, 10.96it/s]\n",
      "100%|██████████| 77/77 [00:06<00:00, 11.53it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00, 10.07it/s]\n",
      "100%|██████████| 77/77 [00:06<00:00, 12.36it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00, 10.49it/s]\n",
      "100%|██████████| 77/77 [00:06<00:00, 11.76it/s]\n",
      "100%|██████████| 77/77 [00:06<00:00, 11.55it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00, 10.32it/s]\n",
      "100%|██████████| 77/77 [00:06<00:00, 12.53it/s]\n",
      "100%|██████████| 77/77 [00:06<00:00, 11.75it/s]\n",
      "100%|██████████| 77/77 [00:06<00:00, 11.08it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00, 10.98it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00, 10.75it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00, 10.98it/s]\n",
      "100%|██████████| 77/77 [00:06<00:00, 11.75it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00, 10.51it/s]\n",
      "100%|██████████| 77/77 [00:06<00:00, 11.90it/s]\n",
      "100%|██████████| 77/77 [00:06<00:00, 12.27it/s]\n",
      "100%|██████████| 77/77 [00:06<00:00, 11.98it/s]\n",
      "100%|██████████| 77/77 [00:06<00:00, 11.26it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00,  9.77it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00,  9.69it/s]\n",
      "100%|██████████| 77/77 [00:06<00:00, 11.79it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00, 10.22it/s]\n",
      "100%|██████████| 77/77 [00:06<00:00, 11.34it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00, 10.74it/s]\n",
      "100%|██████████| 77/77 [00:06<00:00, 11.06it/s]\n",
      "100%|██████████| 77/77 [00:06<00:00, 11.04it/s]\n"
     ]
    }
   ],
   "source": [
    "#print(\"Extraction features from train data\")\n",
    "#trainData = feature_extraction('/content/drive/MyDrive/KTH/DD2119_Speech_Recognition/Labs/data/tidigits/tidigits/disc_4.1.1/tidigits/train')\n",
    "# Save the data to avoid computing it again\n",
    "#np.savez('trainData.npz', trainData=trainData)\n",
    "\n",
    "print(\"Extracting features from test data\")\n",
    "testData = feature_extraction('../tidigits/disc_4.2.1/tidigits/test')\n",
    "np.savez('testData.npz', testData=testData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the path to the files in your Google Drive\n",
    "train_data_path = '/content/trainData.npz'\n",
    "test_data_path = '/content/testData.npz'\n",
    "\n",
    "\n",
    "# Download the files\n",
    "# files.download(train_data_path)\n",
    "# files.download(test_data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 Training and Validation Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitDataByGender(data_dict,gender, train_utterances):\n",
    "  train_data, val_data = [], []\n",
    "  for current_speaker in data_dict[gender].keys():\n",
    "    # if train_data contains 90 or more procent of total gender utterances, remaining data is stored as validation\n",
    "    if len(train_data) >= train_utterances:\n",
    "      #print(f\"len(train_data): {len(train_data)} > {train_utterances} --> Creating val set instead\")\n",
    "      val_data.extend(data_dict[gender][str(current_speaker)])\n",
    "    # Otherwise, we keep adding to train data until we achieve 90%\n",
    "    else:\n",
    "      #print(f\"len(train_data): {len(train_data)} < {train_utterances}\")\n",
    "      train_data.extend(data_dict[gender][str(current_speaker)])\n",
    "  print(f\"train_data: {len(train_data)} \\t val_data: {len(val_data)}\")\n",
    "  return train_data, val_data\n",
    "\n",
    "def splitData(total_data, split=0.1):\n",
    "  data_by_gender = {\"man\":{}, \"woman\": {}}\n",
    "  print(f\"Total_data length = {len(total_data)}\")\n",
    "  for data in total_data:\n",
    "    gender, speakerID, _, _ = tools3.path2info(data[\"filename\"])  # path2info returns tuple (gender, speakerID, digits, repetition)\n",
    "    if speakerID not in data_by_gender[gender]:\n",
    "      data_by_gender[gender][speakerID] = []\n",
    "    data_by_gender[gender][speakerID].append(data)\n",
    "\n",
    "  # Calculate total utterances by summing the lengths of each gender's list\n",
    "  total_male_utterances = sum(len(utterances) for utterances in data_by_gender[\"man\"].values())\n",
    "  total_female_utterances = sum(len(utterances) for utterances in data_by_gender[\"woman\"].values())\n",
    "\n",
    "  train_male_utterances = int(total_male_utterances * (1-split))     # compute how many male utterances to achieve 90%\n",
    "  train_female_utterances = int(total_female_utterances * (1-split)) # compute how many female utterances to achieve 90%\n",
    "  print(f\"total male utterances: {total_male_utterances}\\ntrain_male_utterances: {train_male_utterances}\")\n",
    "  print(f\"total female utterances: {total_female_utterances}\\ntrain_female_utterances: {train_female_utterances}\")\n",
    "\n",
    "  male_train_data, male_val_data = splitDataByGender(data_by_gender, \"man\", train_male_utterances)\n",
    "  female_train_data, female_val_data = splitDataByGender(data_by_gender, \"woman\", train_female_utterances)\n",
    "\n",
    "  train_data, val_data = [], []\n",
    "  train_data.extend(male_train_data)\n",
    "  train_data.extend(female_train_data)\n",
    "  val_data.extend(male_val_data)\n",
    "  val_data.extend(female_val_data)\n",
    "\n",
    "  print(f\"train data has {len(train_data)} elements\")\n",
    "  print(f\"val data has {len(val_data)} elements\")\n",
    "\n",
    "  return train_data, val_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trainData, valData = splitData(trainData)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5 Acoustic Context (Dynamic Features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_context(features, context=3):\n",
    "    \"\"\"\n",
    "    Augments the features by adding context frames around each time step in the feature matrix.\n",
    "\n",
    "    Args:\n",
    "    features (np.array): The original feature matrix where each row is a time step and columns are features.\n",
    "    context (int): The number of frames to include from before and after the current frame.\n",
    "\n",
    "    Returns:\n",
    "    np.array: An augmented feature matrix including context.\n",
    "    \"\"\"\n",
    "    rows, cols = features.shape\n",
    "    context_features = np.zeros((rows, cols * (2 * context + 1)))\n",
    "\n",
    "    for i in range(rows):\n",
    "        for j in range(-context, context + 1):\n",
    "            if 0 <= i + j < rows:\n",
    "                context_features[i, (j + context) * cols: (j + context + 1) * cols] = features[i + j]\n",
    "            else:\n",
    "                # Use mirroring for edge cases\n",
    "                mirrored_index = min(max(0, i + j), rows - 1)\n",
    "                context_features[i, (j + context) * cols: (j + context + 1) * cols] = features[mirrored_index]\n",
    "\n",
    "    return context_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.6 Feature Standardisation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 Phoneme Recognition with Deep Neural Networks\n",
    "\n",
    "## 5.1 Detailed Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Possible Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
