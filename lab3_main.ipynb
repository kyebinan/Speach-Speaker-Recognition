{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import soundfile as sf\n",
    "import os \n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "\n",
    "from Lab3 import lab3_proto as proto3\n",
    "from Lab3 import lab3_tools as tools3\n",
    "from Lab2 import lab2_proto as proto2\n",
    "from Lab2 import lab2_tools as tools2\n",
    "from Lab1 import lab1_proto as proto1\n",
    "from Lab1 import lab1_tools as tools1\n",
    "from Lab2.prondict import prondict "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing the Data for DNN Training \n",
    "\n",
    "## 4.1 Target Class Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ah_0', 'ah_1', 'ah_2', 'ao_0', 'ao_1', 'ao_2', 'ay_0', 'ay_1', 'ay_2', 'eh_0', 'eh_1', 'eh_2', 'ey_0', 'ey_1', 'ey_2', 'f_0', 'f_1', 'f_2', 'ih_0', 'ih_1', 'ih_2', 'iy_0', 'iy_1', 'iy_2', 'k_0', 'k_1', 'k_2', 'n_0', 'n_1', 'n_2', 'ow_0', 'ow_1', 'ow_2', 'r_0', 'r_1', 'r_2', 's_0', 's_1', 's_2', 'sil_0', 'sil_1', 'sil_2', 'sp_0', 't_0', 't_1', 't_2', 'th_0', 'th_1', 'th_2', 'uw_0', 'uw_1', 'uw_2', 'v_0', 'v_1', 'v_2', 'w_0', 'w_1', 'w_2', 'z_0', 'z_1', 'z_2']\n",
      "\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "phoneHMMs = np.load(\"./Lab2/lab2_models_all.npz\", allow_pickle=True)[\"phoneHMMs\"].item()\n",
    "phones = sorted(phoneHMMs.keys())\n",
    "nstates = {phone: phoneHMMs[phone]['means'].shape[0] for phone in phones}\n",
    "stateList = [ph + '_' + str(id) for ph in phones for id in range(nstates[ph])]\n",
    "print(stateList)\n",
    "print()\n",
    "print(stateList.index('ay_2'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Forced Alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wordTrans: ['z', '4', '3']\n",
      "phoneTrans: ['sil', 'z', 'iy', 'r', 'ow', 'sp', 'f', 'ao', 'r', 'sp', 'th', 'r', 'iy', 'sp', 'sil']\n",
      "stateTrans[10]: r_1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_7231/2984385968.py:17: RuntimeWarning: divide by zero encountered in log\n",
      "  viterbiLoglik, viterbiPath = proto2.viterbi(obsloglik, np.log(utteranceHMM['startprob'][:-1]), np.log(utteranceHMM['transmat'][:-1, :-1]), forceFinalState=True)\n"
     ]
    }
   ],
   "source": [
    "# the dataset is not on the github due to the copyright\n",
    "filename = './../tidigits/disc_4.1.1/tidigits/train/man/nw/z43a.wav'\n",
    "samples, samplingrate = tools3.loadAudio(filename)\n",
    "lmfcc = proto1.mfcc(samples)\n",
    "\n",
    "wordTrans = list(tools3.path2info(filename)[2])  # Transcription using words\n",
    "print(f\"wordTrans: {wordTrans}\")\n",
    "\n",
    "phoneTrans = proto3.words2phones(wordTrans, prondict) # Transcription using phonemes\n",
    "print(f\"phoneTrans: {phoneTrans}\")\n",
    "\n",
    "utteranceHMM = proto2.concatHMMs(phoneHMMs, phoneTrans)\n",
    "stateTrans = [phone + '_' + str(stateid) for phone in phoneTrans for stateid in range(nstates[phone])]  # Transcription using states\n",
    "print(f\"stateTrans[10]: {stateTrans[10]}\")\n",
    "\n",
    "obsloglik = tools2.log_multivariate_normal_density_diag(lmfcc, utteranceHMM[\"means\"], utteranceHMM[\"covars\"])\n",
    "viterbiLoglik, viterbiPath = proto2.viterbi(obsloglik, np.log(utteranceHMM['startprob'][:-1]), np.log(utteranceHMM['transmat'][:-1, :-1]), forceFinalState=True)\n",
    "\n",
    "viterbiStateTrans = [stateTrans[state] for state in viterbiPath]\n",
    "\n",
    "trans = tools3.frames2trans(viterbiStateTrans, outfilename='z43a.lab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mspec_res = proto1.mspec(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lmfcc: True\n",
      "Our wordTrans: \n",
      "['z', '4', '3']\n",
      "Correct wordTrans: \n",
      "['z', '4', '3']\n",
      "Our phoneTrans: \n",
      "['sil', 'z', 'iy', 'r', 'ow', 'sp', 'f', 'ao', 'r', 'sp', 'th', 'r', 'iy', 'sp', 'sil']\n",
      "Correct phoneTrans: \n",
      "['sil', 'z', 'iy', 'r', 'ow', 'sp', 'f', 'ao', 'r', 'sp', 'th', 'r', 'iy', 'sp', 'sil']\n",
      "Our stateTrans: \n",
      "['sil_0', 'sil_1', 'sil_2', 'z_0', 'z_1', 'z_2', 'iy_0', 'iy_1', 'iy_2', 'r_0', 'r_1', 'r_2', 'ow_0', 'ow_1', 'ow_2', 'sp_0', 'f_0', 'f_1', 'f_2', 'ao_0', 'ao_1', 'ao_2', 'r_0', 'r_1', 'r_2', 'sp_0', 'th_0', 'th_1', 'th_2', 'r_0', 'r_1', 'r_2', 'iy_0', 'iy_1', 'iy_2', 'sp_0', 'sil_0', 'sil_1', 'sil_2']\n",
      "Correct stateTrans: \n",
      "['sil_0', 'sil_1', 'sil_2', 'z_0', 'z_1', 'z_2', 'iy_0', 'iy_1', 'iy_2', 'r_0', 'r_1', 'r_2', 'ow_0', 'ow_1', 'ow_2', 'sp_0', 'f_0', 'f_1', 'f_2', 'ao_0', 'ao_1', 'ao_2', 'r_0', 'r_1', 'r_2', 'sp_0', 'th_0', 'th_1', 'th_2', 'r_0', 'r_1', 'r_2', 'iy_0', 'iy_1', 'iy_2', 'sp_0', 'sil_0', 'sil_1', 'sil_2']\n",
      "obsloglik: True\n",
      "viterbiLoglik: True\n",
      "viterbiPath: True\n",
      "Our viterbiStateTrans: \n",
      "['sil_0', 'sil_1', 'sil_1', 'sil_1', 'sil_1', 'sil_1', 'sil_1', 'sil_1', 'sil_1', 'sil_1', 'sil_1', 'sil_1', 'sil_1', 'sil_1', 'sil_1', 'sil_1', 'sil_1', 'sil_1', 'sil_1', 'sil_2', 'z_0', 'z_0', 'z_0', 'z_0', 'z_1', 'z_2', 'z_2', 'z_2', 'z_2', 'z_2', 'z_2', 'z_2', 'z_2', 'z_2', 'z_2', 'z_2', 'iy_0', 'iy_0', 'iy_0', 'iy_0', 'iy_0', 'iy_0', 'iy_0', 'iy_0', 'iy_1', 'iy_2', 'r_0', 'r_0', 'r_0', 'r_0', 'r_0', 'r_0', 'r_0', 'r_0', 'r_0', 'r_0', 'r_1', 'r_2', 'ow_0', 'ow_1', 'ow_2', 'ow_2', 'ow_2', 'ow_2', 'ow_2', 'ow_2', 'ow_2', 'ow_2', 'ow_2', 'f_0', 'f_1', 'f_1', 'f_1', 'f_1', 'f_1', 'f_1', 'f_1', 'f_1', 'f_1', 'f_1', 'f_1', 'f_2', 'ao_0', 'ao_1', 'ao_1', 'ao_1', 'ao_1', 'ao_1', 'ao_1', 'ao_1', 'ao_1', 'ao_1', 'ao_1', 'ao_1', 'ao_1', 'ao_1', 'ao_1', 'ao_2', 'ao_2', 'ao_2', 'ao_2', 'ao_2', 'ao_2', 'ao_2', 'ao_2', 'ao_2', 'ao_2', 'ao_2', 'r_0', 'r_0', 'r_0', 'r_1', 'r_2', 'th_0', 'th_0', 'th_0', 'th_0', 'th_0', 'th_0', 'th_0', 'th_0', 'th_0', 'th_0', 'th_1', 'th_1', 'th_1', 'th_2', 'r_0', 'r_0', 'r_0', 'r_0', 'r_0', 'r_0', 'r_0', 'r_0', 'r_0', 'r_1', 'r_2', 'iy_0', 'iy_0', 'iy_0', 'iy_0', 'iy_0', 'iy_0', 'iy_0', 'iy_0', 'iy_0', 'iy_0', 'iy_1', 'iy_1', 'iy_2', 'iy_2', 'iy_2', 'iy_2', 'iy_2', 'iy_2', 'iy_2', 'iy_2', 'sil_0', 'sil_0', 'sil_0', 'sil_0', 'sil_0', 'sil_0', 'sil_0', 'sil_0', 'sil_0', 'sil_0', 'sil_0', 'sil_0', 'sil_0', 'sil_0', 'sil_0', 'sil_0', 'sil_0', 'sil_0', 'sil_1', 'sil_2']\n",
      "Correct viterbiStateTrans: \n",
      "['sil_0', 'sil_1', 'sil_1', 'sil_1', 'sil_1', 'sil_1', 'sil_1', 'sil_1', 'sil_1', 'sil_1', 'sil_1', 'sil_1', 'sil_1', 'sil_1', 'sil_1', 'sil_1', 'sil_1', 'sil_1', 'sil_1', 'sil_2', 'z_0', 'z_0', 'z_0', 'z_0', 'z_1', 'z_2', 'z_2', 'z_2', 'z_2', 'z_2', 'z_2', 'z_2', 'z_2', 'z_2', 'z_2', 'z_2', 'iy_0', 'iy_0', 'iy_0', 'iy_0', 'iy_0', 'iy_0', 'iy_0', 'iy_0', 'iy_1', 'iy_2', 'r_0', 'r_0', 'r_0', 'r_0', 'r_0', 'r_0', 'r_0', 'r_0', 'r_0', 'r_0', 'r_1', 'r_2', 'ow_0', 'ow_1', 'ow_2', 'ow_2', 'ow_2', 'ow_2', 'ow_2', 'ow_2', 'ow_2', 'ow_2', 'ow_2', 'f_0', 'f_1', 'f_1', 'f_1', 'f_1', 'f_1', 'f_1', 'f_1', 'f_1', 'f_1', 'f_1', 'f_1', 'f_2', 'ao_0', 'ao_1', 'ao_1', 'ao_1', 'ao_1', 'ao_1', 'ao_1', 'ao_1', 'ao_1', 'ao_1', 'ao_1', 'ao_1', 'ao_1', 'ao_1', 'ao_1', 'ao_2', 'ao_2', 'ao_2', 'ao_2', 'ao_2', 'ao_2', 'ao_2', 'ao_2', 'ao_2', 'ao_2', 'ao_2', 'r_0', 'r_0', 'r_0', 'r_1', 'r_2', 'th_0', 'th_0', 'th_0', 'th_0', 'th_0', 'th_0', 'th_0', 'th_0', 'th_0', 'th_0', 'th_1', 'th_1', 'th_1', 'th_2', 'r_0', 'r_0', 'r_0', 'r_0', 'r_0', 'r_0', 'r_0', 'r_0', 'r_0', 'r_1', 'r_2', 'iy_0', 'iy_0', 'iy_0', 'iy_0', 'iy_0', 'iy_0', 'iy_0', 'iy_0', 'iy_0', 'iy_0', 'iy_1', 'iy_1', 'iy_2', 'iy_2', 'iy_2', 'iy_2', 'iy_2', 'iy_2', 'iy_2', 'iy_2', 'sil_0', 'sil_0', 'sil_0', 'sil_0', 'sil_0', 'sil_0', 'sil_0', 'sil_0', 'sil_0', 'sil_0', 'sil_0', 'sil_0', 'sil_0', 'sil_0', 'sil_0', 'sil_0', 'sil_0', 'sil_0', 'sil_1', 'sil_2']\n"
     ]
    }
   ],
   "source": [
    "example = np.load(\"./Lab3/lab3_example.npz\", allow_pickle=True)[\"example\"].item()\n",
    "# Compare each variable with its corresponding value in the example dictionary\n",
    "\n",
    "print(f\"lmfcc: {np.allclose(lmfcc, example['lmfcc'])}\")\n",
    "print(f\"Our wordTrans: \\n{wordTrans}\\nCorrect wordTrans: \\n{example['wordTrans']}\")\n",
    "print(f\"Our phoneTrans: \\n{phoneTrans}\\nCorrect phoneTrans: \\n{example['phoneTrans']}\")\n",
    "print(f\"Our stateTrans: \\n{stateTrans}\\nCorrect stateTrans: \\n{example['stateTrans']}\")\n",
    "print(f\"obsloglik: {np.allclose(obsloglik, example['obsloglik'])}\")\n",
    "print(f\"viterbiLoglik: {np.allclose(viterbiLoglik, example['viterbiLoglik'])}\")\n",
    "print(f\"viterbiPath: {np.allclose(viterbiPath, example['viterbiPath'])}\")\n",
    "print(f\"Our viterbiStateTrans: \\n{viterbiStateTrans}\\nCorrect viterbiStateTrans: \\n{example['viterbiStateTrans']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Feature Extraction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_extraction(path):\n",
    "  data = []\n",
    "\n",
    "  for root, dirs, files in os.walk(path):\n",
    "    for file in tqdm(files):\n",
    "      if file.endswith('.wav'):\n",
    "        filename = os.path.join(root, file)\n",
    "        samples, samplingrate = tools3.loadAudio(filename)\n",
    "\n",
    "        lmfcc = proto1.mfcc(samples) # Features used for HMM & DNN\n",
    "        mspec_res = proto1.mspec(samples) # Features used for DNN\n",
    "\n",
    "        wordTrans = list(tools3.path2info(filename))[2]  # Transcription using words\n",
    "        phoneTrans = proto3.words2phones(wordTrans, prondict) # Transcription using phonemes\n",
    "        targets = proto3.forcedAlignment(lmfcc, phoneHMMs, phoneTrans) # Align states to each utterance\n",
    "\n",
    "        # converting targets to indices to save memory\n",
    "        target_idx = np.array([stateList.index(target) for target in targets])\n",
    "\n",
    "        data.append({'filename': filename, 'lmfcc': lmfcc,'mspec': mspec_res, 'targets': target_idx})\n",
    "\n",
    "  return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extraction features from train data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "  0%|          | 0/77 [00:00<?, ?it/s]/home/yohan/Documents/Cantique_des_Cantiques/Project/Speach/Speach-Speaker-Recognition/Lab3/lab3_proto.py:49: RuntimeWarning: divide by zero encountered in log\n",
      "  _, viterbi_path = viterbi(obslogik, np.log(utteranceHMM[\"startprob\"][:-1]), np.log(utteranceHMM[\"transmat\"][:-1, :-1]), forceFinalState=True)\n",
      "100%|██████████| 77/77 [00:08<00:00,  9.61it/s]\n",
      "100%|██████████| 77/77 [00:08<00:00,  9.00it/s]\n",
      "100%|██████████| 77/77 [00:11<00:00,  6.84it/s]\n",
      "100%|██████████| 77/77 [00:10<00:00,  7.44it/s]\n",
      "100%|██████████| 77/77 [00:11<00:00,  6.82it/s]\n",
      "100%|██████████| 77/77 [00:12<00:00,  6.23it/s]\n",
      "100%|██████████| 77/77 [00:08<00:00,  9.08it/s]\n",
      "100%|██████████| 77/77 [00:08<00:00,  9.16it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00, 10.95it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00,  9.98it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00, 10.50it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00, 10.09it/s]\n",
      "100%|██████████| 77/77 [00:08<00:00,  9.50it/s]\n",
      "100%|██████████| 77/77 [00:06<00:00, 11.20it/s]\n",
      "100%|██████████| 77/77 [00:09<00:00,  8.40it/s]\n",
      "100%|██████████| 77/77 [00:08<00:00,  9.54it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00, 10.31it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00,  9.92it/s]\n",
      "100%|██████████| 77/77 [00:08<00:00,  8.97it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00,  9.79it/s]\n",
      "100%|██████████| 77/77 [00:08<00:00,  9.59it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00,  9.97it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00, 10.01it/s]\n",
      "100%|██████████| 77/77 [00:09<00:00,  8.54it/s]\n",
      "100%|██████████| 77/77 [00:08<00:00,  9.00it/s]\n",
      "100%|██████████| 76/76 [00:06<00:00, 10.94it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00, 10.65it/s]\n",
      "100%|██████████| 77/77 [00:08<00:00,  9.21it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00,  9.75it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00,  9.70it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00,  9.71it/s]\n",
      "100%|██████████| 77/77 [00:08<00:00,  9.17it/s]\n",
      "100%|██████████| 77/77 [00:08<00:00,  9.45it/s]\n",
      "100%|██████████| 77/77 [00:08<00:00,  9.07it/s]\n",
      "100%|██████████| 77/77 [00:08<00:00,  9.34it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00,  9.72it/s]\n",
      "100%|██████████| 77/77 [00:08<00:00,  8.61it/s]\n",
      "100%|██████████| 77/77 [00:06<00:00, 11.17it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00,  9.91it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00, 10.65it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00, 10.66it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00, 10.43it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00, 10.88it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00, 10.49it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00, 10.43it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00, 10.81it/s]\n",
      "100%|██████████| 77/77 [00:06<00:00, 11.19it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00,  9.66it/s]\n",
      "100%|██████████| 77/77 [00:08<00:00,  9.49it/s]\n",
      "100%|██████████| 77/77 [00:06<00:00, 11.07it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00, 10.95it/s]\n",
      "100%|██████████| 77/77 [00:08<00:00,  9.51it/s]\n",
      "100%|██████████| 77/77 [00:08<00:00,  9.39it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00, 10.63it/s]\n",
      "100%|██████████| 77/77 [00:08<00:00,  8.78it/s]\n",
      "100%|██████████| 77/77 [00:09<00:00,  8.38it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00, 10.70it/s]\n",
      "0it [00:00, ?it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00, 10.95it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00, 10.19it/s]\n",
      "100%|██████████| 77/77 [00:08<00:00,  8.88it/s]\n",
      "100%|██████████| 77/77 [00:06<00:00, 11.31it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00, 10.76it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00,  9.71it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00,  9.63it/s]\n",
      "100%|██████████| 77/77 [00:08<00:00,  8.94it/s]\n",
      "100%|██████████| 77/77 [00:06<00:00, 11.18it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00, 10.95it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00, 10.87it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00, 10.24it/s]\n",
      "100%|██████████| 77/77 [00:06<00:00, 11.09it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00,  9.80it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00,  9.86it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00,  9.93it/s]\n",
      "100%|██████████| 77/77 [00:08<00:00,  9.04it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00, 10.00it/s]\n",
      "100%|██████████| 77/77 [00:06<00:00, 11.05it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00,  9.73it/s]\n",
      "100%|██████████| 77/77 [00:06<00:00, 11.23it/s]\n",
      "100%|██████████| 77/77 [00:08<00:00,  9.28it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00, 10.37it/s]\n",
      "100%|██████████| 77/77 [00:06<00:00, 11.13it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00,  9.82it/s]\n",
      "100%|██████████| 77/77 [00:09<00:00,  8.44it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00, 10.63it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00, 10.52it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00, 10.41it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00, 10.02it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00,  9.84it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00,  9.89it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00, 10.76it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00,  9.66it/s]\n",
      "100%|██████████| 77/77 [00:08<00:00,  9.45it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00, 10.13it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00, 10.11it/s]\n",
      "100%|██████████| 77/77 [00:06<00:00, 11.12it/s]\n",
      "100%|██████████| 77/77 [00:06<00:00, 12.37it/s]\n",
      "100%|██████████| 77/77 [00:08<00:00,  8.64it/s]\n",
      "100%|██████████| 77/77 [00:08<00:00,  9.62it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00, 10.04it/s]\n",
      "100%|██████████| 77/77 [00:06<00:00, 11.32it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00, 10.79it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00,  9.78it/s]\n",
      "100%|██████████| 77/77 [00:06<00:00, 11.01it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00, 10.64it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00, 10.42it/s]\n",
      "100%|██████████| 77/77 [00:08<00:00,  9.09it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00, 10.94it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00, 10.32it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00, 10.35it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00, 10.49it/s]\n",
      "100%|██████████| 77/77 [00:06<00:00, 11.76it/s]\n",
      "100%|██████████| 77/77 [00:06<00:00, 11.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting features from test data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00, 10.99it/s]\n",
      "100%|██████████| 77/77 [00:08<00:00,  9.46it/s]\n",
      "100%|██████████| 77/77 [00:08<00:00,  9.29it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00, 10.67it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00, 10.13it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00, 10.74it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00, 10.40it/s]\n",
      "100%|██████████| 77/77 [00:08<00:00,  9.59it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00, 10.68it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00, 10.46it/s]\n",
      "100%|██████████| 77/77 [00:08<00:00,  9.15it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00, 10.61it/s]\n",
      "100%|██████████| 77/77 [00:08<00:00,  8.66it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00, 10.01it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00,  9.68it/s]\n",
      "100%|██████████| 77/77 [00:08<00:00,  8.84it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00, 10.76it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00, 10.38it/s]\n",
      "100%|██████████| 77/77 [00:06<00:00, 11.21it/s]\n",
      "100%|██████████| 77/77 [00:08<00:00,  9.45it/s]\n",
      "100%|██████████| 77/77 [00:08<00:00,  9.10it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00, 10.61it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00, 10.24it/s]\n",
      "100%|██████████| 77/77 [00:08<00:00,  8.89it/s]\n",
      "100%|██████████| 77/77 [00:08<00:00,  9.38it/s]\n",
      "100%|██████████| 77/77 [00:08<00:00,  9.27it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00, 10.62it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00, 10.49it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00, 10.25it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00,  9.65it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00, 10.63it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00,  9.79it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00,  9.87it/s]\n",
      "100%|██████████| 77/77 [00:08<00:00,  9.37it/s]\n",
      "100%|██████████| 77/77 [00:08<00:00,  9.29it/s]\n",
      "100%|██████████| 77/77 [00:06<00:00, 11.55it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00, 10.48it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00, 10.24it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00,  9.91it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00, 10.48it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00,  9.67it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00, 10.64it/s]\n",
      "100%|██████████| 77/77 [00:08<00:00,  8.87it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00, 10.11it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00,  9.64it/s]\n",
      "100%|██████████| 77/77 [00:06<00:00, 11.13it/s]\n",
      "100%|██████████| 77/77 [00:08<00:00,  9.05it/s]\n",
      "100%|██████████| 77/77 [00:08<00:00,  9.53it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00,  9.77it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00, 10.05it/s]\n",
      "100%|██████████| 77/77 [00:08<00:00,  8.86it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00, 10.14it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00, 10.00it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00,  9.95it/s]\n",
      "100%|██████████| 77/77 [00:09<00:00,  8.54it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00, 10.20it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00, 10.27it/s]\n",
      "0it [00:00, ?it/s]\n",
      "100%|██████████| 77/77 [00:06<00:00, 11.19it/s]\n",
      "100%|██████████| 77/77 [00:08<00:00,  8.88it/s]\n",
      "100%|██████████| 77/77 [00:08<00:00,  9.26it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00,  9.92it/s]\n",
      "100%|██████████| 77/77 [00:08<00:00,  9.17it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00, 10.25it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00,  9.69it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00, 10.52it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00, 10.73it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00,  9.77it/s]\n",
      "100%|██████████| 77/77 [00:08<00:00,  8.97it/s]\n",
      "100%|██████████| 77/77 [00:08<00:00,  8.63it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00, 10.03it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00, 10.08it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00,  9.73it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00, 10.59it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00,  9.67it/s]\n",
      "100%|██████████| 77/77 [00:09<00:00,  8.20it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00, 10.64it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00,  9.77it/s]\n",
      "100%|██████████| 77/77 [00:06<00:00, 11.23it/s]\n",
      "100%|██████████| 77/77 [00:06<00:00, 11.77it/s]\n",
      "100%|██████████| 77/77 [00:09<00:00,  8.47it/s]\n",
      "100%|██████████| 77/77 [00:06<00:00, 12.11it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00,  9.66it/s]\n",
      "100%|██████████| 76/76 [00:06<00:00, 11.28it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00, 10.54it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00,  9.63it/s]\n",
      "100%|██████████| 77/77 [00:08<00:00,  9.31it/s]\n",
      "100%|██████████| 77/77 [00:08<00:00,  9.55it/s]\n",
      "100%|██████████| 77/77 [00:08<00:00,  9.19it/s]\n",
      "100%|██████████| 77/77 [00:06<00:00, 11.61it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00, 10.23it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00, 10.83it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00,  9.74it/s]\n",
      "100%|██████████| 77/77 [00:08<00:00,  9.03it/s]\n",
      "100%|██████████| 77/77 [00:06<00:00, 11.41it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00, 10.35it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00,  9.72it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00,  9.79it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00,  9.84it/s]\n",
      "100%|██████████| 77/77 [00:08<00:00,  9.52it/s]\n",
      "100%|██████████| 77/77 [00:08<00:00,  8.84it/s]\n",
      "100%|██████████| 77/77 [00:08<00:00,  9.25it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00, 10.60it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00, 10.52it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00, 10.69it/s]\n",
      "100%|██████████| 77/77 [00:08<00:00,  9.53it/s]\n",
      "100%|██████████| 77/77 [00:08<00:00,  8.97it/s]\n",
      "100%|██████████| 77/77 [00:08<00:00,  9.17it/s]\n",
      "100%|██████████| 77/77 [00:06<00:00, 11.19it/s]\n",
      "100%|██████████| 77/77 [00:08<00:00,  8.97it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00, 10.44it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00,  9.86it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00, 10.59it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00,  9.72it/s]\n"
     ]
    }
   ],
   "source": [
    "print(\"Extraction features from train data\")\n",
    "trainData = feature_extraction('../tidigits/disc_4.1.1/tidigits/train')\n",
    "# Save the data to avoid computing it again\n",
    "np.savez('trainData.npz', trainData=trainData)\n",
    "\n",
    "print(\"Extracting features from test data\")\n",
    "testData = feature_extraction('../tidigits/disc_4.2.1/tidigits/test')\n",
    "np.savez('testData.npz', testData=testData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 Training and Validation Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitDataByGender(data_dict,gender, train_utterances):\n",
    "  train_data, val_data = [], []\n",
    "  for current_speaker in data_dict[gender].keys():\n",
    "    # if train_data contains 90 or more procent of total gender utterances, remaining data is stored as validation\n",
    "    if len(train_data) >= train_utterances:\n",
    "      #print(f\"len(train_data): {len(train_data)} > {train_utterances} --> Creating val set instead\")\n",
    "      val_data.extend(data_dict[gender][str(current_speaker)])\n",
    "    # Otherwise, we keep adding to train data until we achieve 90%\n",
    "    else:\n",
    "      #print(f\"len(train_data): {len(train_data)} < {train_utterances}\")\n",
    "      train_data.extend(data_dict[gender][str(current_speaker)])\n",
    "  print(f\"train_data: {len(train_data)} \\t val_data: {len(val_data)}\")\n",
    "  return train_data, val_data\n",
    "\n",
    "def splitData(total_data, split=0.1):\n",
    "  data_by_gender = {\"man\":{}, \"woman\": {}}\n",
    "  print(f\"Total_data length = {len(total_data)}\")\n",
    "  for data in total_data:\n",
    "    gender, speakerID, _, _ = tools3.path2info(data[\"filename\"])  # path2info returns tuple (gender, speakerID, digits, repetition)\n",
    "    if speakerID not in data_by_gender[gender]:\n",
    "      data_by_gender[gender][speakerID] = []\n",
    "    data_by_gender[gender][speakerID].append(data)\n",
    "\n",
    "  # Calculate total utterances by summing the lengths of each gender's list\n",
    "  total_male_utterances = sum(len(utterances) for utterances in data_by_gender[\"man\"].values())\n",
    "  total_female_utterances = sum(len(utterances) for utterances in data_by_gender[\"woman\"].values())\n",
    "\n",
    "  train_male_utterances = int(total_male_utterances * (1-split))     # compute how many male utterances to achieve 90%\n",
    "  train_female_utterances = int(total_female_utterances * (1-split)) # compute how many female utterances to achieve 90%\n",
    "  print(f\"total male utterances: {total_male_utterances}\\ntrain_male_utterances: {train_male_utterances}\")\n",
    "  print(f\"total female utterances: {total_female_utterances}\\ntrain_female_utterances: {train_female_utterances}\")\n",
    "\n",
    "  male_train_data, male_val_data = splitDataByGender(data_by_gender, \"man\", train_male_utterances)\n",
    "  female_train_data, female_val_data = splitDataByGender(data_by_gender, \"woman\", train_female_utterances)\n",
    "\n",
    "  train_data, val_data = [], []\n",
    "  train_data.extend(male_train_data)\n",
    "  train_data.extend(female_train_data)\n",
    "  val_data.extend(male_val_data)\n",
    "  val_data.extend(female_val_data)\n",
    "\n",
    "  print(f\"train data has {len(train_data)} elements\")\n",
    "  print(f\"val data has {len(val_data)} elements\")\n",
    "\n",
    "  return train_data, val_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainData has 8623 elements\n",
      "key: filename - ../tidigits/disc_4.1.1/tidigits/train/woman/cl/za.wav\n",
      "key: lmfcc - [[ 103.1708824   -82.31110635   14.29338738 ...  -38.65502294\n",
      "  -155.95559352  -48.36368006]\n",
      " [ 109.42118969  -11.18161627  117.72133428 ...    8.63922846\n",
      "   -40.50538129   21.4327149 ]\n",
      " [ 116.09423861  -46.88554472   25.04088065 ...  -75.15169037\n",
      "   -16.42606443   67.31586026]\n",
      " ...\n",
      " [ 203.16342222  109.37593821   16.40208643 ...  -27.98419695\n",
      "    -7.0104208     4.57830005]\n",
      " [ 164.15952204   87.7355812    85.18308825 ...  117.86320303\n",
      "    15.88054796   27.40947736]\n",
      " [ 170.64996493  107.48479268  124.78120348 ...   -5.24549154\n",
      "    49.41318624   10.30882344]]\n",
      "key: mspec - [[-0.12084544  1.03020631  0.46282056 ...  2.09551978  2.27765554\n",
      "   2.41654246]\n",
      " [ 2.54187435  3.06084006  3.07138977 ...  2.16677831  1.97251175\n",
      "   2.2343435 ]\n",
      " [ 1.52510378  2.79239405  1.89996556 ...  1.91766317  1.89914442\n",
      "   2.66109756]\n",
      " ...\n",
      " [ 4.40261508  3.47242441  2.87425463 ...  1.88805656  1.82493374\n",
      "   1.75817315]\n",
      " [ 5.22065891  3.47026996  1.56461146 ...  1.61069173  1.81613402\n",
      "   1.75268973]\n",
      " [ 4.78665577  4.24782695  3.10876783 ...  2.07996067  2.32817091\n",
      "   2.87288276]]\n",
      "key: targets - [39 39 39 39 39 39 39 39 39 39 39 39 39 39 39 39 39 39 39 39 39 39 39 39\n",
      " 39 39 39 39 39 39 39 40 40 40 40 40 40 40 40 41 41 41 41 41 58 58 58 58\n",
      " 58 58 58 59 59 59 59 60 60 21 21 21 21 21 21 21 21 21 22 23 33 34 34 34\n",
      " 34 34 34 34 34 34 34 34 34 34 34 34 34 34 35 30 30 30 30 30 30 30 30 31\n",
      " 31 31 31 32 32 32 32 32 32 32 39 39 39 39 39 39 39 39 39 39 39 39 39 39\n",
      " 39 39 39 39 39 39 39 39 39 39 40 41]\n"
     ]
    }
   ],
   "source": [
    "train_data_path = \"./trainData.npz\"\n",
    "test_data_path = \"./testData.npz\"\n",
    "trainData = np.load(train_data_path, allow_pickle=True)[\"trainData\"]\n",
    "testData = np.load(test_data_path, allow_pickle=True)[\"testData\"]\n",
    "\n",
    "print(f\"trainData has {len(trainData)} elements\")\n",
    "for key in trainData[0].keys():\n",
    "  print(f\"key: {key} - {trainData[0][key]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5 Acoustic Context (Dynamic Features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_context(features, context=3):\n",
    "    \"\"\"\n",
    "    Augments the features by adding context frames around each time step in the feature matrix.\n",
    "\n",
    "    Args:\n",
    "    features (np.array): The original feature matrix where each row is a time step and columns are features.\n",
    "    context (int): The number of frames to include from before and after the current frame.\n",
    "\n",
    "    Returns:\n",
    "    np.array: An augmented feature matrix including context.\n",
    "    \"\"\"\n",
    "    rows, cols = features.shape\n",
    "    context_features = np.zeros((rows, cols * (2 * context + 1)))\n",
    "\n",
    "    for i in range(rows):\n",
    "        for j in range(-context, context + 1):\n",
    "            if 0 <= i + j < rows:\n",
    "                context_features[i, (j + context) * cols: (j + context + 1) * cols] = features[i + j]\n",
    "            else:\n",
    "                # Use mirroring for edge cases\n",
    "                mirrored_index = min(max(0, i + j), rows - 1)\n",
    "                context_features[i, (j + context) * cols: (j + context + 1) * cols] = features[mirrored_index]\n",
    "\n",
    "    return context_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[103.1708824 , -82.31110635,  14.29338738, ...,   5.21621609,\n",
       "        -62.65111028, -48.97697333],\n",
       "       [103.1708824 , -82.31110635,  14.29338738, ..., -95.22756163,\n",
       "         16.2071291 ,  40.90530715],\n",
       "       [103.1708824 , -82.31110635,  14.29338738, ..., -72.15412933,\n",
       "        -66.9767564 , -71.81263328],\n",
       "       ...,\n",
       "       [153.06616189,  26.72406375,  29.80793612, ...,  -5.24549154,\n",
       "         49.41318624,  10.30882344],\n",
       "       [128.26044834,  37.93693872,  99.15316669, ...,  -5.24549154,\n",
       "         49.41318624,  10.30882344],\n",
       "       [177.653294  ,  91.29562855,  43.55066378, ...,  -5.24549154,\n",
       "         49.41318624,  10.30882344]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add_context(trainData[0][\"lmfcc\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total_data length = 8623\n",
      "total male utterances: 4235\n",
      "train_male_utterances: 3811\n",
      "total female utterances: 4388\n",
      "train_female_utterances: 3949\n",
      "train_data: 3850 \t val_data: 385\n",
      "train_data: 4003 \t val_data: 385\n",
      "train data has 7853 elements\n",
      "val data has 770 elements\n"
     ]
    }
   ],
   "source": [
    "trainData, valData = splitData(trainData)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data:\n",
      "N: 1366280\n",
      "D_lmfcc: 13 \tD_mspec: 40\n",
      "Validation data:\n",
      "N: 140777\n",
      "D_lmfcc: 13 \tD_mspec: 40\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "print(\"Training data:\")\n",
    "N = sum([len(data['targets']) for data in trainData])\n",
    "D_lmfcc = trainData[0][\"lmfcc\"].shape[1]\n",
    "D_mspec = trainData[0][\"mspec\"].shape[1]\n",
    "print(f\"N: {N}\")\n",
    "print(f\"D_lmfcc: {D_lmfcc} \\tD_mspec: {D_mspec}\")\n",
    "\n",
    "# Val\n",
    "print(\"Validation data:\")\n",
    "N = sum([len(data['targets']) for data in valData])\n",
    "D_lmfcc = valData[0][\"lmfcc\"].shape[1]\n",
    "D_mspec = valData[0][\"mspec\"].shape[1]\n",
    "print(f\"N: {N}\")\n",
    "print(f\"D_lmfcc: {D_lmfcc} \\tD_mspec: {D_mspec}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.6 Feature Standardisation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preproccsed all data\n",
      "trainLMFCCX: (1366280, 91) \t trainMSPECX: (1366280, 40) \t trainY: torch.Size([1366280, 61])\n",
      "valLMFCCX: (140777, 91) \t valMSPECX: (140777, 40) \t valY: torch.Size([140777, 61])\n",
      "testLMFCCX: (1526682, 91) \t testMSPECX: (1526682, 40) \t testY: torch.Size([1526682, 61])\n"
     ]
    }
   ],
   "source": [
    "def preprocessing(totalData, _num_classes, context=True):\n",
    "    Ns = [len(data['targets']) for data in totalData]\n",
    "    N = sum(Ns)\n",
    "    D_lmfcc = totalData[0][\"lmfcc\"].shape[1] * (7 if context else 1)\n",
    "    D_mspec = totalData[0][\"mspec\"].shape[1]\n",
    "    X_lmfcc = np.zeros((N, D_lmfcc)).astype(\"float32\")\n",
    "    X_mspec = np.zeros((N, D_mspec)).astype(\"float32\")\n",
    "    Y = np.zeros(N, dtype=int)\n",
    "\n",
    "    startPointer = 0\n",
    "    for i, data in enumerate(totalData):\n",
    "        lmfcc = data[\"lmfcc\"]\n",
    "        if context:\n",
    "            lmfcc = add_context(lmfcc, context=3)\n",
    "        X_lmfcc[startPointer: startPointer + Ns[i]] = lmfcc\n",
    "        X_mspec[startPointer: startPointer + Ns[i]] = data[\"mspec\"]\n",
    "        Y[startPointer: startPointer + Ns[i]] = data[\"targets\"]\n",
    "        startPointer += Ns[i]\n",
    "\n",
    "    Y = F.one_hot(torch.tensor(Y), num_classes=_num_classes)\n",
    "    return X_lmfcc, X_mspec, Y\n",
    "\n",
    "# Creating scalers to standardize the data\n",
    "scalerLMFCC = StandardScaler()\n",
    "scalerMSPEC = StandardScaler()\n",
    "\n",
    "# Preprocessing the data\n",
    "trainLMFCCX, trainMSPECX, trainY = preprocessing(trainData, len(stateList))\n",
    "valLMFCCX, valMSPECX, valY = preprocessing(valData, len(stateList))\n",
    "testLMFCCX, testMSPECX, testY = preprocessing(testData, len(stateList))\n",
    "\n",
    "# Standardizing the data\n",
    "scalerLMFCC.fit(trainLMFCCX)\n",
    "trainLMFCCX = scalerLMFCC.transform(trainLMFCCX)\n",
    "valLMFCCX = scalerLMFCC.transform(valLMFCCX)\n",
    "testLMFCCX = scalerLMFCC.transform(testLMFCCX)\n",
    "\n",
    "scalerMSPEC.fit(trainMSPECX)\n",
    "trainMSPECX = scalerMSPEC.transform(trainMSPECX)\n",
    "valMSPECX = scalerMSPEC.transform(valMSPECX)\n",
    "testMSPECX = scalerMSPEC.transform(testMSPECX)\n",
    "\n",
    "print(f\"Preproccsed all data\")\n",
    "print(f\"trainLMFCCX: {trainLMFCCX.shape} \\t trainMSPECX: {trainMSPECX.shape} \\t trainY: {trainY.shape}\")\n",
    "print(f\"valLMFCCX: {valLMFCCX.shape} \\t valMSPECX: {valMSPECX.shape} \\t valY: {valY.shape}\")\n",
    "print(f\"testLMFCCX: {testLMFCCX.shape} \\t testMSPECX: {testMSPECX.shape} \\t testY: {testY.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 Phoneme Recognition with Deep Neural Networks\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FC_ReLU(in_dim: int,\n",
    "            out_dim: int) -> nn.Sequential:\n",
    "  \"\"\"\n",
    "    Creates a fully connected layer with ReLU activation.\n",
    "\n",
    "    Parameters:\n",
    "    - in_dim (int): The number of input features.\n",
    "    - out_dim (int): The number of output features.\n",
    "\n",
    "    Returns:\n",
    "    - nn.Sequential: A sequential container with a linear layer, batch normalization, and ReLU activation.\n",
    "  \"\"\"\n",
    "  return nn.Sequential(\n",
    "      nn.Linear(in_features=in_dim, out_features=out_dim),\n",
    "      nn.BatchNorm1d(out_dim),\n",
    "      nn.ReLU()\n",
    "  )\n",
    "\n",
    "def FC_Sigmoid(in_dim: int,\n",
    "               out_dim: int) -> nn.Sequential:\n",
    "  \"\"\"\n",
    "    Creates a fully connected layer with Sigmoid activation.\n",
    "\n",
    "    Parameters:\n",
    "    - in_dim (int): The number of input features.\n",
    "    - out_dim (int): The number of output features.\n",
    "\n",
    "    Returns:\n",
    "    - nn.Sequential: A sequential container with a linear layer, batch normalization, and Sigmoid activation.\n",
    "  \"\"\"\n",
    "  return nn.Sequential(\n",
    "      nn.Linear(in_features=in_dim, out_features=out_dim),\n",
    "      nn.BatchNorm1d(out_dim),\n",
    "      nn.Sigmoid()\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: unspecified launch failure\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [30]\u001b[0m, in \u001b[0;36m<cell line: 88>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     86\u001b[0m hidden_dims \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m256\u001b[39m,\u001b[38;5;241m256\u001b[39m,\u001b[38;5;241m256\u001b[39m]\n\u001b[1;32m     87\u001b[0m output_dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(stateList) \u001b[38;5;66;03m# classes\u001b[39;00m\n\u001b[0;32m---> 88\u001b[0m net \u001b[38;5;241m=\u001b[39m \u001b[43mNet\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden_dims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_dim\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28mprint\u001b[39m(net)\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnumber of prameters:\u001b[39m\u001b[38;5;124m'\u001b[39m, count_parameters(net))\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:907\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    903\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    904\u001b[0m                     non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[1;32m    905\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, non_blocking)\n\u001b[0;32m--> 907\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:578\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    576\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn):\n\u001b[1;32m    577\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 578\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    580\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    581\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    582\u001b[0m             \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    583\u001b[0m             \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    588\u001b[0m             \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    589\u001b[0m             \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:578\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    576\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn):\n\u001b[1;32m    577\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 578\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    580\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    581\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    582\u001b[0m             \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    583\u001b[0m             \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    588\u001b[0m             \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    589\u001b[0m             \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:578\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    576\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn):\n\u001b[1;32m    577\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 578\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    580\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    581\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    582\u001b[0m             \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    583\u001b[0m             \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    588\u001b[0m             \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    589\u001b[0m             \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:601\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    597\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    598\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    599\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    600\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 601\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    602\u001b[0m should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    603\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:905\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    902\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m    903\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    904\u001b[0m                 non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[0;32m--> 905\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: unspecified launch failure\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1."
     ]
    }
   ],
   "source": [
    "# define the neural network architecture\n",
    "class Net(torch.nn.Module):\n",
    "  def __init__(self,\n",
    "               input_dim: int,\n",
    "               hidden_dims: list[int],\n",
    "               output_dim: int,\n",
    "               active_type: str =\"ReLU\"):\n",
    "    super().__init__()\n",
    "    self.input_dim = input_dim\n",
    "    self.hidden_dims = hidden_dims\n",
    "    self.output_dim = output_dim\n",
    "    self.active_type = active_type\n",
    "\n",
    "    self.create_network()\n",
    "    self.weight_init()\n",
    "\n",
    "  def create_network(self) -> None:\n",
    "    \"\"\"\n",
    "      Constructs the neural network architecture.\n",
    "\n",
    "      This method creates a sequential container with fully connected layers and the specified activation functions.\n",
    "    \"\"\"\n",
    "    modules = []\n",
    "    input_dim = self.input_dim\n",
    "    if self.hidden_dims: # Check if hidden_dims is not empty\n",
    "      for hidden_dim in self.hidden_dims:\n",
    "        if self.active_type == \"ReLU\":\n",
    "          modules.append(FC_ReLU(input_dim, hidden_dim))\n",
    "        elif self.active_type == \"Sigmoid\":\n",
    "          modules.append(FC_Sigmoid(input_dim, hidden_dim))\n",
    "\n",
    "        input_dim = hidden_dim\n",
    "\n",
    "    modules.append(nn.Linear(in_features=input_dim, out_features=self.output_dim))\n",
    "\n",
    "    self.network = nn.Sequential(*modules)\n",
    "    self.layers = len(self.hidden_dims) + 1\n",
    "\n",
    "  def weight_init(self) -> None:\n",
    "    \"\"\"\n",
    "      Initializes the weights of the network.\n",
    "\n",
    "      This method applies He initialization for ReLU activations and Xavier initialization for Sigmoid activations.\n",
    "    \"\"\"\n",
    "    for m in self.modules():\n",
    "        if isinstance(m, (nn.Linear)):\n",
    "          if self.active_type == \"ReLU\":\n",
    "            # He initialization\n",
    "            nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='relu')\n",
    "          elif self.active_type == \"Sigmoid\":\n",
    "            # Xavier initalization\n",
    "            nn.init.xavier_uniform_(m.weight)\n",
    "\n",
    "  def forward(self,\n",
    "              X: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "      Forward pass through the network.\n",
    "\n",
    "      Parameters:\n",
    "      - X (torch.Tensor): Input tensor.\n",
    "\n",
    "      Returns:\n",
    "      - torch.Tensor: Output tensor after applying softmax activation.\n",
    "    \"\"\"\n",
    "    output = self.network(X)\n",
    "    return F.softmax(output, dim=1)\n",
    "\n",
    "def count_parameters(net):\n",
    "    return sum(p.numel() for p in net.parameters() if p.requires_grad)\n",
    "\n",
    "featureType = \"lmfcc\"\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "if featureType == \"lmfcc\":\n",
    "  # prepare/load the data into tensors\n",
    "  train_x, val_x, test_x = trainLMFCCX, valLMFCCX, testLMFCCX\n",
    "else:\n",
    "  train_x, val_x, test_x = trainMSPECX, valMSPECX, testLMFCCX\n",
    "\n",
    "train_y, val_y, test_y = trainY.float(), valY.float(), testY.float()\n",
    "\n",
    "# instantiate the network and print the structure\n",
    "input_dim = train_x.shape[1]\n",
    "hidden_dims = [256,256,256]\n",
    "output_dim = len(stateList) # classes\n",
    "net = Net(input_dim, hidden_dims, output_dim).to(device)\n",
    "print(net)\n",
    "print('number of prameters:', count_parameters(net))\n",
    "\n",
    "# define your loss criterion (see https://pytorch.org/docs/stable/nn.html#loss-functions)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# define the optimizer\n",
    "optimizer = torch.optim.Adam(net.parameters())\n",
    "\n",
    "batch_size = 128    # they had 256 but said we could lower it to reduce compute\n",
    "\n",
    "# create the data loaders for training and validation sets\n",
    "train_dataset = torch.utils.data.TensorDataset(torch.Tensor(train_x), train_y)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_dataset = torch.utils.data.TensorDataset(torch.Tensor(val_x), val_y)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_dataset = torch.utils.data.TensorDataset(torch.Tensor(test_x), test_y)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# train the network\n",
    "num_epochs = 50   # default 100 but starting with lower to see if it works\n",
    "\n",
    "# Path to store everything\n",
    "base_path = \"/content/drive/MyDrive/KTH/DD2119_Speech_Recognition/Labs/models\"\n",
    "\n",
    "current_time = datetime.now().strftime(\"%H%M\")\n",
    "model_id = f\"model_{num_epochs}_epochs_{batch_size}_batch_size_{hidden_dims}_hidden_{current_time}\"\n",
    "model_subfolder = os.path.join(base_path, model_id)\n",
    "os.makedirs(model_subfolder, exist_ok=True)\n",
    "\n",
    "# Define the paths for saving the model and logs\n",
    "model_path = os.path.join(model_subfolder, f'model_dict.pt')\n",
    "logs_path = os.path.join(model_subfolder, 'logs')\n",
    "\n",
    "# setup logging so that you can follow training using TensorBoard (see https://pytorch.org/docs/stable/tensorboard.html)\n",
    "writer = SummaryWriter(log_dir = logs_path)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "  net.train()\n",
    "  train_loss, train_acc = 0.0, 0.0\n",
    "  i = 0\n",
    "  for inputs, labels in tqdm(train_loader, desc=\"Training Progress\", unit=\"batch\"):\n",
    "    inputs, labels = inputs.to(device), labels.to(device)\n",
    "    # zero the parameter gradients\n",
    "    optimizer.zero_grad()\n",
    "    # forward + backward + optimize\n",
    "    outputs = net(inputs)\n",
    "    loss = criterion(outputs, labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    # accumulate the training loss and acc\n",
    "    train_loss += loss.item()\n",
    "    _, predicted = torch.max(outputs, 1)    # extract the most probable label prediction\n",
    "    _, correct = torch.max(labels, 1)       # extract the most correct label prediction\n",
    "    train_acc += (predicted == correct).sum().item() # count the sum of all label predictions that were correct\n",
    "\n",
    "  net.eval()\n",
    "  with torch.no_grad():\n",
    "      val_loss, val_acc = 0.0, 0.0\n",
    "      for inputs, labels in tqdm(val_loader, desc=\"Validation Progress\", unit=\"batch\"):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        val_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs, 1)    # extract the most probable label prediction\n",
    "        _, correct = torch.max(labels, 1)       # extract the most correct label prediction\n",
    "        val_acc += (predicted == correct).sum().item() # count the sum of all label predictions that were correct\n",
    "\n",
    "\n",
    "  # print the epoch loss\n",
    "  train_loss /= len(train_loader)\n",
    "  val_loss /= len(val_loader)\n",
    "  train_acc /= len(train_loader)\n",
    "  val_acc /= len(val_loader)\n",
    "\n",
    "  print(f'Epoch {epoch}: train_loss={train_loss}, val_loss={val_loss}, train_acc={train_acc}, val_acc={val_acc}')\n",
    "  writer.add_scalars('loss',{'train':train_loss,'val':val_loss},epoch)\n",
    "  writer.add_scalars('acc', {'train':train_acc,'val':val_acc}, epoch)\n",
    "\n",
    "net.eval()\n",
    "with torch.no_grad():\n",
    "    test_loss, test_acc = 0.0, 0.0\n",
    "    for inputs, labels in tqdm(test_loader, desc=\"Test Progress\", unit=\"batch\"):\n",
    "      inputs, labels = inputs.to(device), labels.to(device)\n",
    "      outputs = net(inputs)\n",
    "      loss = criterion(outputs, labels)\n",
    "      test_loss += loss.item()\n",
    "      _, predicted = torch.max(outputs, 1)    # extract the most probable label prediction\n",
    "      _, correct = torch.max(labels, 1)       # extract the most correct label prediction\n",
    "      test_acc += (predicted == correct).sum().item() # count the sum of all label predictions that were correct\n",
    "\n",
    "test_loss /= len(test_loader)\n",
    "test_acc /= len(test_loader)\n",
    "\n",
    "print(f'Test Result: test_loss={test_loss} \\t test_acc={test_acc}')\n",
    "\n",
    "# save the trained network\n",
    "torch.save(net.state_dict(), model_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  5.1 Detailed Evaluation\n",
    "\n",
    "### 1. frame-by-frame at the state level : count the number of frames (time steps) that were correctly classified over the total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "10.2\n",
      "NVIDIA GeForce RTX 3060 Laptop GPU\n",
      "Using device: cpu\n",
      "Net(\n",
      "  (network): Sequential(\n",
      "    (0): Sequential(\n",
      "      (0): Linear(in_features=91, out_features=256, bias=True)\n",
      "      (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU()\n",
      "    )\n",
      "    (1): Sequential(\n",
      "      (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "      (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU()\n",
      "    )\n",
      "    (2): Sequential(\n",
      "      (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "      (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU()\n",
      "    )\n",
      "    (3): Linear(in_features=256, out_features=61, bias=True)\n",
      "  )\n",
      ")\n",
      "number of prameters: 172349\n"
     ]
    }
   ],
   "source": [
    "# PREPARE THE DATA \n",
    "\n",
    "featureType = \"lmfcc\"\n",
    "\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.version.cuda)\n",
    "print(torch.cuda.get_device_name(0))\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(\"cpu\")  # Switch to CPU to test the model setup\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "if featureType == \"lmfcc\":\n",
    "  # prepare/load the data into tensors\n",
    "  train_x, val_x, test_x = trainLMFCCX, valLMFCCX, testLMFCCX\n",
    "else:\n",
    "  train_x, val_x, test_x = trainMSPECX, valMSPECX, testLMFCCX\n",
    "\n",
    "train_y, val_y, test_y = trainY.float(), valY.float(), testY.float()\n",
    "\n",
    "# instantiate the network and print the structure\n",
    "input_dim = train_x.shape[1]\n",
    "hidden_dims = [256,256,256]\n",
    "output_dim = len(stateList) # classes\n",
    "net = Net(input_dim, hidden_dims, output_dim).to(device)\n",
    "print(net)\n",
    "print('number of prameters:', count_parameters(net))\n",
    "\n",
    "# define your loss criterion (see https://pytorch.org/docs/stable/nn.html#loss-functions)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# define the optimizer\n",
    "optimizer = torch.optim.Adam(net.parameters())\n",
    "\n",
    "batch_size = 256  # they had 256 but said we could lower it to reduce compute\n",
    "\n",
    "# create the data loaders for training and validation sets\n",
    "train_dataset = torch.utils.data.TensorDataset(torch.Tensor(train_x), train_y)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_dataset = torch.utils.data.TensorDataset(torch.Tensor(val_x), val_y)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_dataset = torch.utils.data.TensorDataset(torch.Tensor(test_x), test_y)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# train the network\n",
    "num_epochs = 100   # default 100 but starting with lower to see if it works\n",
    "\n",
    "# Path to store everything\n",
    "base_path = \"./Lab3/models\"\n",
    "\n",
    "current_time = datetime.now().strftime(\"%H%M\")\n",
    "model_id = f\"model_{num_epochs}_epochs_{batch_size}_batch_size_{hidden_dims}_hidden_{current_time}\"\n",
    "model_subfolder = os.path.join(base_path, model_id)\n",
    "os.makedirs(model_subfolder, exist_ok=True)\n",
    "\n",
    "# Define the paths for saving the model and logs\n",
    "model_path = os.path.join(model_subfolder, f'model_dict.pt')\n",
    "logs_path = os.path.join(model_subfolder, 'logs')\n",
    "\n",
    "# setup logging so that you can follow training using TensorBoard (see https://pytorch.org/docs/stable/tensorboard.html)\n",
    "writer = SummaryWriter(log_dir = logs_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Validation Loss: 3.5287418365478516\n",
      "Epoch 1: Validation Loss: 3.509099006652832\n",
      "Epoch 2: Validation Loss: 3.4941606521606445\n",
      "Epoch 3: Validation Loss: 3.4784600734710693\n",
      "Epoch 4: Validation Loss: 3.471487283706665\n",
      "Epoch 5: Validation Loss: 3.4680838584899902\n",
      "Epoch 6: Validation Loss: 3.468064785003662\n",
      "Epoch 7: Validation Loss: 3.4679453372955322\n",
      "Epoch 8: Validation Loss: 3.4656403064727783\n",
      "Epoch 9: Validation Loss: 3.467432737350464\n",
      "Epoch 10: Validation Loss: 3.4614789485931396\n",
      "Epoch 11: Validation Loss: 3.461686849594116\n",
      "Epoch 12: Validation Loss: 3.4600114822387695\n",
      "Epoch 13: Validation Loss: 3.458801746368408\n",
      "Epoch 14: Validation Loss: 3.456779956817627\n",
      "Epoch 15: Validation Loss: 3.4627604484558105\n",
      "Epoch 16: Validation Loss: 3.4597432613372803\n",
      "Epoch 17: Validation Loss: 3.459841728210449\n",
      "Epoch 18: Validation Loss: 3.4559948444366455\n",
      "Epoch 19: Validation Loss: 3.4546213150024414\n",
      "Epoch 20: Validation Loss: 3.4613115787506104\n",
      "Epoch 21: Validation Loss: 3.4551355838775635\n",
      "Epoch 22: Validation Loss: 3.450871229171753\n",
      "Epoch 23: Validation Loss: 3.449403762817383\n",
      "Epoch 24: Validation Loss: 3.4469306468963623\n",
      "Epoch 25: Validation Loss: 3.4456796646118164\n",
      "Epoch 26: Validation Loss: 3.4518189430236816\n",
      "Epoch 27: Validation Loss: 3.447153329849243\n",
      "Epoch 28: Validation Loss: 3.445401191711426\n",
      "Epoch 29: Validation Loss: 3.444962501525879\n",
      "Epoch 30: Validation Loss: 3.4458680152893066\n",
      "Epoch 31: Validation Loss: 3.442920207977295\n",
      "Epoch 32: Validation Loss: 3.444927215576172\n",
      "Epoch 33: Validation Loss: 3.44527006149292\n",
      "Epoch 34: Validation Loss: 3.4459950923919678\n",
      "Epoch 35: Validation Loss: 3.4401469230651855\n",
      "Epoch 36: Validation Loss: 3.435433864593506\n",
      "Epoch 37: Validation Loss: 3.440223217010498\n",
      "Epoch 38: Validation Loss: 3.438054323196411\n",
      "Epoch 39: Validation Loss: 3.4388351440429688\n",
      "Epoch 40: Validation Loss: 3.4403774738311768\n",
      "Epoch 41: Validation Loss: 3.4391379356384277\n",
      "Epoch 42: Validation Loss: 3.4374372959136963\n",
      "Epoch 43: Validation Loss: 3.4371917247772217\n",
      "Epoch 44: Validation Loss: 3.4381072521209717\n",
      "Epoch 45: Validation Loss: 3.437657117843628\n",
      "Epoch 46: Validation Loss: 3.4316041469573975\n",
      "Epoch 47: Validation Loss: 3.4358019828796387\n",
      "Epoch 48: Validation Loss: 3.43550443649292\n",
      "Epoch 49: Validation Loss: 3.438267230987549\n",
      "Epoch 50: Validation Loss: 3.4353227615356445\n",
      "Epoch 51: Validation Loss: 3.4342448711395264\n",
      "Epoch 52: Validation Loss: 3.4362382888793945\n",
      "Epoch 53: Validation Loss: 3.434662103652954\n",
      "Epoch 54: Validation Loss: 3.436223030090332\n",
      "Epoch 55: Validation Loss: 3.4364101886749268\n",
      "Epoch 56: Validation Loss: 3.4353511333465576\n",
      "Epoch 57: Validation Loss: 3.4387834072113037\n",
      "Epoch 58: Validation Loss: 3.435159683227539\n",
      "Epoch 59: Validation Loss: 3.4401538372039795\n",
      "Epoch 60: Validation Loss: 3.438915252685547\n",
      "Epoch 61: Validation Loss: 3.437218189239502\n",
      "Epoch 62: Validation Loss: 3.436448097229004\n",
      "Epoch 63: Validation Loss: 3.435269594192505\n",
      "Epoch 64: Validation Loss: 3.4392786026000977\n",
      "Epoch 65: Validation Loss: 3.4388182163238525\n",
      "Epoch 66: Validation Loss: 3.4351766109466553\n",
      "Epoch 67: Validation Loss: 3.434807062149048\n",
      "Epoch 68: Validation Loss: 3.435516834259033\n",
      "Epoch 69: Validation Loss: 3.4354984760284424\n",
      "Epoch 70: Validation Loss: 3.436724901199341\n",
      "Epoch 71: Validation Loss: 3.4379241466522217\n",
      "Epoch 72: Validation Loss: 3.437102794647217\n",
      "Epoch 73: Validation Loss: 3.4352850914001465\n",
      "Epoch 74: Validation Loss: 3.4353554248809814\n",
      "Epoch 75: Validation Loss: 3.4373443126678467\n",
      "Epoch 76: Validation Loss: 3.436429023742676\n",
      "Epoch 77: Validation Loss: 3.4375617504119873\n",
      "Epoch 78: Validation Loss: 3.437748908996582\n",
      "Epoch 79: Validation Loss: 3.4353044033050537\n",
      "Epoch 80: Validation Loss: 3.4343862533569336\n",
      "Epoch 81: Validation Loss: 3.437981605529785\n",
      "Epoch 82: Validation Loss: 3.435560941696167\n",
      "Epoch 83: Validation Loss: 3.4331774711608887\n",
      "Epoch 84: Validation Loss: 3.438417673110962\n",
      "Epoch 85: Validation Loss: 3.4340567588806152\n",
      "Epoch 86: Validation Loss: 3.436161756515503\n",
      "Epoch 87: Validation Loss: 3.4361631870269775\n",
      "Epoch 88: Validation Loss: 3.4350764751434326\n",
      "Epoch 89: Validation Loss: 3.433859348297119\n",
      "Epoch 90: Validation Loss: 3.434295654296875\n",
      "Epoch 91: Validation Loss: 3.436518907546997\n",
      "Epoch 92: Validation Loss: 3.434598922729492\n",
      "Epoch 93: Validation Loss: 3.434663772583008\n",
      "Epoch 94: Validation Loss: 3.432648181915283\n",
      "Epoch 95: Validation Loss: 3.4350452423095703\n",
      "Epoch 96: Validation Loss: 3.4337480068206787\n",
      "Epoch 97: Validation Loss: 3.434133291244507\n",
      "Epoch 98: Validation Loss: 3.4365041255950928\n",
      "Epoch 99: Validation Loss: 3.434386730194092\n"
     ]
    }
   ],
   "source": [
    "# TRAINING THE MODEL \n",
    "# use 91 instead of 13 because we use context in the data check part 4.5 \n",
    "model = Net(input_dim=91, hidden_dims=[256, 256, 256], output_dim=output_dim, active_type=\"ReLU\")\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for inputs, targets in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_loss = sum(criterion(model(inputs), targets) for inputs, targets in val_loader) / len(val_loader)\n",
    "    print(f'Epoch {epoch}: Validation Loss: {val_loss.item()}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EVALUATE THE MODEL\n",
    "\n",
    "model.eval()\n",
    "test_loss = 0\n",
    "correct = 0\n",
    "with torch.no_grad():\n",
    "    for inputs, targets in test_loader:\n",
    "        outputs = model(inputs)\n",
    "        test_loss += criterion(outputs, targets).item()\n",
    "        pred = outputs.argmax(dim=1, keepdim=True)\n",
    "        correct += pred.eq(targets.view_as(pred)).sum().item()\n",
    "\n",
    "test_loss /= len(test_loader.dataset)\n",
    "print(f'Test set: Average loss: {test_loss:.4f}, Accuracy: {correct}/{len(test_loader.dataset)} ({100. * correct / len(test_loader.dataset):.0f}%)')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. frame-by-frame at the phoneme level : same as 1., but merge all states that correspond to the same phoneme, for example ox_0, ox_1 and ox_2 are merged to ox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def evaluate_model(model, loader):\n",
    "    model.eval()\n",
    "    all_outputs = []\n",
    "    all_targets = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in loader:\n",
    "            outputs = model(inputs)\n",
    "            all_outputs.append(outputs)\n",
    "            all_targets.append(targets)\n",
    "    return torch.cat(all_outputs), torch.cat(all_targets)\n",
    "\n",
    "\n",
    "outputs, targets = evaluate_model(model, test_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_posteriors(outputs, targets, idx):\n",
    "    \"\"\"\n",
    "    Plot the posterior probabilities for each phoneme class for a single example.\n",
    "    \n",
    "    Args:\n",
    "    outputs (torch.Tensor): The output from the model, typically log probabilities.\n",
    "    targets (torch.Tensor): The actual targets (labels).\n",
    "    idx (int): Index of the utterance to plot.\n",
    "    \"\"\"\n",
    "    # Convert outputs to probabilities\n",
    "    probabilities = torch.exp(outputs[idx])  # Assuming output is log_softmax\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.plot(probabilities.cpu().numpy(), label='Posterior probabilities')\n",
    "    plt.scatter([i for i in range(len(probabilities))], targets[idx].cpu().numpy(), color='red', label='True Labels', marker='x')  # Assuming target is one-hot or indices\n",
    "    plt.title('Posterior Probabilities vs. True Labels')\n",
    "    plt.xlabel('Time Steps')\n",
    "    plt.ylabel('Probabilities')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Example usage: plot for the first test example\n",
    "plot_posteriors(outputs, targets, 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def frame_by_frame_accuracy(outputs, targets):\n",
    "    \"\"\"\n",
    "    Compute the frame-by-frame accuracy for state and phoneme levels.\n",
    "    \n",
    "    Args:\n",
    "    outputs (torch.Tensor): Logits from the model.\n",
    "    targets (torch.Tensor): True labels.\n",
    "\n",
    "    Returns:\n",
    "    float: Accuracy at the state level.\n",
    "    \"\"\"\n",
    "    # Assuming outputs are raw logits, apply softmax to convert to probabilities\n",
    "    predicted_states = torch.argmax(torch.softmax(outputs, dim=1), dim=1)\n",
    "    correct = (predicted_states == targets).float().sum()\n",
    "    total = targets.shape[0]\n",
    "    accuracy = correct / total\n",
    "    return accuracy.item()\n",
    "\n",
    "# Example of usage\n",
    "accuracy = frame_by_frame_accuracy(outputs, targets)\n",
    "print(f\"Frame-by-frame accuracy at state level: {accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_confusion_matrix(outputs, targets, num_classes):\n",
    "    \"\"\"\n",
    "    Compute confusion matrix for evaluating classification performance.\n",
    "    \n",
    "    Args:\n",
    "    outputs (torch.Tensor): Model outputs, assumed to be logits.\n",
    "    targets (torch.Tensor): Ground truth labels.\n",
    "    num_classes (int): Number of classes.\n",
    "\n",
    "    Returns:\n",
    "    np.ndarray: A confusion matrix.\n",
    "    \"\"\"\n",
    "    preds = torch.argmax(torch.softmax(outputs, dim=1), dim=1)\n",
    "    conf_matrix = confusion_matrix(targets.cpu().numpy(), preds.cpu().numpy(), labels=list(range(num_classes)))\n",
    "    return conf_matrix\n",
    "\n",
    "# Example of usage\n",
    "conf_matrix = compute_confusion_matrix(outputs, targets, num_classes)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Levenshtein import distance as levenshtein_distance\n",
    "\n",
    "def edit_distance_evaluation(outputs, targets):\n",
    "    \"\"\"\n",
    "    Compute the edit distance at the state level for evaluating classification performance.\n",
    "    \n",
    "    Args:\n",
    "    outputs (torch.Tensor): Model outputs, assumed to be logits.\n",
    "    targets (torch.Tensor): Ground truth labels.\n",
    "\n",
    "    Returns:\n",
    "    float: Normalized edit distance, representing the Phone Error Rate (PER).\n",
    "    \"\"\"\n",
    "    predicted_states = torch.argmax(torch.softmax(outputs, dim=1), dim=1)\n",
    "    predicted_sequence = predicted_states.cpu().numpy().tolist()\n",
    "    target_sequence = targets.cpu().numpy().tolist()\n",
    "\n",
    "    # Convert state sequences to string format for edit distance calculation\n",
    "    predicted_str = ''.join(map(str, predicted_sequence))\n",
    "    target_str = ''.join(map(str, target_sequence))\n",
    "\n",
    "    # Calculate edit distance\n",
    "    edit_dist = levenshtein_distance(predicted_str, target_str)\n",
    "    per = edit_dist / len(target_sequence)\n",
    "    return per\n",
    "\n",
    "# Example of usage\n",
    "per = edit_distance_evaluation(outputs, targets)\n",
    "print(f\"Phone Error Rate (PER): {per:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
