{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import soundfile as sf\n",
    "import os \n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "\n",
    "from Lab3 import lab3_proto as proto3\n",
    "from Lab3 import lab3_tools as tools3\n",
    "from Lab2 import lab2_proto as proto2\n",
    "from Lab2 import lab2_tools as tools2\n",
    "from Lab1 import lab1_proto as proto1\n",
    "from Lab1 import lab1_tools as tools1\n",
    "from Lab2.prondict import prondict "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing the Data for DNN Training \n",
    "\n",
    "## 4.1 Target Class Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ah_0', 'ah_1', 'ah_2', 'ao_0', 'ao_1', 'ao_2', 'ay_0', 'ay_1', 'ay_2', 'eh_0', 'eh_1', 'eh_2', 'ey_0', 'ey_1', 'ey_2', 'f_0', 'f_1', 'f_2', 'ih_0', 'ih_1', 'ih_2', 'iy_0', 'iy_1', 'iy_2', 'k_0', 'k_1', 'k_2', 'n_0', 'n_1', 'n_2', 'ow_0', 'ow_1', 'ow_2', 'r_0', 'r_1', 'r_2', 's_0', 's_1', 's_2', 'sil_0', 'sil_1', 'sil_2', 'sp_0', 't_0', 't_1', 't_2', 'th_0', 'th_1', 'th_2', 'uw_0', 'uw_1', 'uw_2', 'v_0', 'v_1', 'v_2', 'w_0', 'w_1', 'w_2', 'z_0', 'z_1', 'z_2']\n",
      "\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "phoneHMMs = np.load(\"./Lab2/lab2_models_all.npz\", allow_pickle=True)[\"phoneHMMs\"].item()\n",
    "phones = sorted(phoneHMMs.keys())\n",
    "nstates = {phone: phoneHMMs[phone]['means'].shape[0] for phone in phones}\n",
    "stateList = [ph + '_' + str(id) for ph in phones for id in range(nstates[ph])]\n",
    "print(stateList)\n",
    "print()\n",
    "print(stateList.index('ay_2'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Forced Alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wordTrans: ['z', '4', '3']\n",
      "phoneTrans: ['sil', 'z', 'iy', 'r', 'ow', 'sp', 'f', 'ao', 'r', 'sp', 'th', 'r', 'iy', 'sp', 'sil']\n",
      "stateTrans[10]: r_1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_719280/2984385968.py:17: RuntimeWarning: divide by zero encountered in log\n",
      "  viterbiLoglik, viterbiPath = proto2.viterbi(obsloglik, np.log(utteranceHMM['startprob'][:-1]), np.log(utteranceHMM['transmat'][:-1, :-1]), forceFinalState=True)\n"
     ]
    }
   ],
   "source": [
    "# the dataset is not on the github due to the copyright\n",
    "filename = './../tidigits/disc_4.1.1/tidigits/train/man/nw/z43a.wav'\n",
    "samples, samplingrate = tools3.loadAudio(filename)\n",
    "lmfcc = proto1.mfcc(samples)\n",
    "\n",
    "wordTrans = list(tools3.path2info(filename)[2])  # Transcription using words\n",
    "print(f\"wordTrans: {wordTrans}\")\n",
    "\n",
    "phoneTrans = proto3.words2phones(wordTrans, prondict) # Transcription using phonemes\n",
    "print(f\"phoneTrans: {phoneTrans}\")\n",
    "\n",
    "utteranceHMM = proto2.concatHMMs(phoneHMMs, phoneTrans)\n",
    "stateTrans = [phone + '_' + str(stateid) for phone in phoneTrans for stateid in range(nstates[phone])]  # Transcription using states\n",
    "print(f\"stateTrans[10]: {stateTrans[10]}\")\n",
    "\n",
    "obsloglik = tools2.log_multivariate_normal_density_diag(lmfcc, utteranceHMM[\"means\"], utteranceHMM[\"covars\"])\n",
    "viterbiLoglik, viterbiPath = proto2.viterbi(obsloglik, np.log(utteranceHMM['startprob'][:-1]), np.log(utteranceHMM['transmat'][:-1, :-1]), forceFinalState=True)\n",
    "\n",
    "viterbiStateTrans = [stateTrans[state] for state in viterbiPath]\n",
    "\n",
    "trans = tools3.frames2trans(viterbiStateTrans, outfilename='z43a.lab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mspec_res = proto1.mspec(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lmfcc: True\n",
      "Our wordTrans: \n",
      "['z', '4', '3']\n",
      "Correct wordTrans: \n",
      "['z', '4', '3']\n",
      "Our phoneTrans: \n",
      "['sil', 'z', 'iy', 'r', 'ow', 'sp', 'f', 'ao', 'r', 'sp', 'th', 'r', 'iy', 'sp', 'sil']\n",
      "Correct phoneTrans: \n",
      "['sil', 'z', 'iy', 'r', 'ow', 'sp', 'f', 'ao', 'r', 'sp', 'th', 'r', 'iy', 'sp', 'sil']\n",
      "Our stateTrans: \n",
      "['sil_0', 'sil_1', 'sil_2', 'z_0', 'z_1', 'z_2', 'iy_0', 'iy_1', 'iy_2', 'r_0', 'r_1', 'r_2', 'ow_0', 'ow_1', 'ow_2', 'sp_0', 'f_0', 'f_1', 'f_2', 'ao_0', 'ao_1', 'ao_2', 'r_0', 'r_1', 'r_2', 'sp_0', 'th_0', 'th_1', 'th_2', 'r_0', 'r_1', 'r_2', 'iy_0', 'iy_1', 'iy_2', 'sp_0', 'sil_0', 'sil_1', 'sil_2']\n",
      "Correct stateTrans: \n",
      "['sil_0', 'sil_1', 'sil_2', 'z_0', 'z_1', 'z_2', 'iy_0', 'iy_1', 'iy_2', 'r_0', 'r_1', 'r_2', 'ow_0', 'ow_1', 'ow_2', 'sp_0', 'f_0', 'f_1', 'f_2', 'ao_0', 'ao_1', 'ao_2', 'r_0', 'r_1', 'r_2', 'sp_0', 'th_0', 'th_1', 'th_2', 'r_0', 'r_1', 'r_2', 'iy_0', 'iy_1', 'iy_2', 'sp_0', 'sil_0', 'sil_1', 'sil_2']\n",
      "obsloglik: True\n",
      "viterbiLoglik: True\n",
      "viterbiPath: True\n",
      "Our viterbiStateTrans: \n",
      "['sil_0', 'sil_1', 'sil_1', 'sil_1', 'sil_1', 'sil_1', 'sil_1', 'sil_1', 'sil_1', 'sil_1', 'sil_1', 'sil_1', 'sil_1', 'sil_1', 'sil_1', 'sil_1', 'sil_1', 'sil_1', 'sil_1', 'sil_2', 'z_0', 'z_0', 'z_0', 'z_0', 'z_1', 'z_2', 'z_2', 'z_2', 'z_2', 'z_2', 'z_2', 'z_2', 'z_2', 'z_2', 'z_2', 'z_2', 'iy_0', 'iy_0', 'iy_0', 'iy_0', 'iy_0', 'iy_0', 'iy_0', 'iy_0', 'iy_1', 'iy_2', 'r_0', 'r_0', 'r_0', 'r_0', 'r_0', 'r_0', 'r_0', 'r_0', 'r_0', 'r_0', 'r_1', 'r_2', 'ow_0', 'ow_1', 'ow_2', 'ow_2', 'ow_2', 'ow_2', 'ow_2', 'ow_2', 'ow_2', 'ow_2', 'ow_2', 'f_0', 'f_1', 'f_1', 'f_1', 'f_1', 'f_1', 'f_1', 'f_1', 'f_1', 'f_1', 'f_1', 'f_1', 'f_2', 'ao_0', 'ao_1', 'ao_1', 'ao_1', 'ao_1', 'ao_1', 'ao_1', 'ao_1', 'ao_1', 'ao_1', 'ao_1', 'ao_1', 'ao_1', 'ao_1', 'ao_1', 'ao_2', 'ao_2', 'ao_2', 'ao_2', 'ao_2', 'ao_2', 'ao_2', 'ao_2', 'ao_2', 'ao_2', 'ao_2', 'r_0', 'r_0', 'r_0', 'r_1', 'r_2', 'th_0', 'th_0', 'th_0', 'th_0', 'th_0', 'th_0', 'th_0', 'th_0', 'th_0', 'th_0', 'th_1', 'th_1', 'th_1', 'th_2', 'r_0', 'r_0', 'r_0', 'r_0', 'r_0', 'r_0', 'r_0', 'r_0', 'r_0', 'r_1', 'r_2', 'iy_0', 'iy_0', 'iy_0', 'iy_0', 'iy_0', 'iy_0', 'iy_0', 'iy_0', 'iy_0', 'iy_0', 'iy_1', 'iy_1', 'iy_2', 'iy_2', 'iy_2', 'iy_2', 'iy_2', 'iy_2', 'iy_2', 'iy_2', 'sil_0', 'sil_0', 'sil_0', 'sil_0', 'sil_0', 'sil_0', 'sil_0', 'sil_0', 'sil_0', 'sil_0', 'sil_0', 'sil_0', 'sil_0', 'sil_0', 'sil_0', 'sil_0', 'sil_0', 'sil_0', 'sil_1', 'sil_2']\n",
      "Correct viterbiStateTrans: \n",
      "['sil_0', 'sil_1', 'sil_1', 'sil_1', 'sil_1', 'sil_1', 'sil_1', 'sil_1', 'sil_1', 'sil_1', 'sil_1', 'sil_1', 'sil_1', 'sil_1', 'sil_1', 'sil_1', 'sil_1', 'sil_1', 'sil_1', 'sil_2', 'z_0', 'z_0', 'z_0', 'z_0', 'z_1', 'z_2', 'z_2', 'z_2', 'z_2', 'z_2', 'z_2', 'z_2', 'z_2', 'z_2', 'z_2', 'z_2', 'iy_0', 'iy_0', 'iy_0', 'iy_0', 'iy_0', 'iy_0', 'iy_0', 'iy_0', 'iy_1', 'iy_2', 'r_0', 'r_0', 'r_0', 'r_0', 'r_0', 'r_0', 'r_0', 'r_0', 'r_0', 'r_0', 'r_1', 'r_2', 'ow_0', 'ow_1', 'ow_2', 'ow_2', 'ow_2', 'ow_2', 'ow_2', 'ow_2', 'ow_2', 'ow_2', 'ow_2', 'f_0', 'f_1', 'f_1', 'f_1', 'f_1', 'f_1', 'f_1', 'f_1', 'f_1', 'f_1', 'f_1', 'f_1', 'f_2', 'ao_0', 'ao_1', 'ao_1', 'ao_1', 'ao_1', 'ao_1', 'ao_1', 'ao_1', 'ao_1', 'ao_1', 'ao_1', 'ao_1', 'ao_1', 'ao_1', 'ao_1', 'ao_2', 'ao_2', 'ao_2', 'ao_2', 'ao_2', 'ao_2', 'ao_2', 'ao_2', 'ao_2', 'ao_2', 'ao_2', 'r_0', 'r_0', 'r_0', 'r_1', 'r_2', 'th_0', 'th_0', 'th_0', 'th_0', 'th_0', 'th_0', 'th_0', 'th_0', 'th_0', 'th_0', 'th_1', 'th_1', 'th_1', 'th_2', 'r_0', 'r_0', 'r_0', 'r_0', 'r_0', 'r_0', 'r_0', 'r_0', 'r_0', 'r_1', 'r_2', 'iy_0', 'iy_0', 'iy_0', 'iy_0', 'iy_0', 'iy_0', 'iy_0', 'iy_0', 'iy_0', 'iy_0', 'iy_1', 'iy_1', 'iy_2', 'iy_2', 'iy_2', 'iy_2', 'iy_2', 'iy_2', 'iy_2', 'iy_2', 'sil_0', 'sil_0', 'sil_0', 'sil_0', 'sil_0', 'sil_0', 'sil_0', 'sil_0', 'sil_0', 'sil_0', 'sil_0', 'sil_0', 'sil_0', 'sil_0', 'sil_0', 'sil_0', 'sil_0', 'sil_0', 'sil_1', 'sil_2']\n"
     ]
    }
   ],
   "source": [
    "example = np.load(\"./Lab3/lab3_example.npz\", allow_pickle=True)[\"example\"].item()\n",
    "# Compare each variable with its corresponding value in the example dictionary\n",
    "\n",
    "print(f\"lmfcc: {np.allclose(lmfcc, example['lmfcc'])}\")\n",
    "print(f\"Our wordTrans: \\n{wordTrans}\\nCorrect wordTrans: \\n{example['wordTrans']}\")\n",
    "print(f\"Our phoneTrans: \\n{phoneTrans}\\nCorrect phoneTrans: \\n{example['phoneTrans']}\")\n",
    "print(f\"Our stateTrans: \\n{stateTrans}\\nCorrect stateTrans: \\n{example['stateTrans']}\")\n",
    "print(f\"obsloglik: {np.allclose(obsloglik, example['obsloglik'])}\")\n",
    "print(f\"viterbiLoglik: {np.allclose(viterbiLoglik, example['viterbiLoglik'])}\")\n",
    "print(f\"viterbiPath: {np.allclose(viterbiPath, example['viterbiPath'])}\")\n",
    "print(f\"Our viterbiStateTrans: \\n{viterbiStateTrans}\\nCorrect viterbiStateTrans: \\n{example['viterbiStateTrans']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Feature Extraction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_extraction(path):\n",
    "  data = []\n",
    "\n",
    "  for root, dirs, files in os.walk(path):\n",
    "    for file in tqdm(files):\n",
    "      if file.endswith('.wav'):\n",
    "        filename = os.path.join(root, file)\n",
    "        samples, samplingrate = tools3.loadAudio(filename)\n",
    "\n",
    "        lmfcc = proto1.mfcc(samples) # Features used for HMM & DNN\n",
    "        mspec_res = proto1.mspec(samples) # Features used for DNN\n",
    "\n",
    "        wordTrans = list(tools3.path2info(filename))[2]  # Transcription using words\n",
    "        phoneTrans = proto3.words2phones(wordTrans, prondict) # Transcription using phonemes\n",
    "        targets = proto3.forcedAlignment(lmfcc, phoneHMMs, phoneTrans) # Align states to each utterance\n",
    "\n",
    "        # converting targets to indices to save memory\n",
    "        target_idx = np.array([stateList.index(target) for target in targets])\n",
    "\n",
    "        data.append({'filename': filename, 'lmfcc': lmfcc,'mspec': mspec_res, 'targets': target_idx})\n",
    "\n",
    "  return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extraction features from train data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "  0%|          | 0/77 [00:00<?, ?it/s]/home/yohan/Documents/Cantique_des_Cantiques/Project/Speach/Speach-Speaker-Recognition/Lab3/lab3_proto.py:49: RuntimeWarning: divide by zero encountered in log\n",
      "  _, viterbi_path = viterbi(obslogik, np.log(utteranceHMM[\"startprob\"][:-1]), np.log(utteranceHMM[\"transmat\"][:-1, :-1]), forceFinalState=True)\n",
      "100%|██████████| 77/77 [00:08<00:00,  9.61it/s]\n",
      "100%|██████████| 77/77 [00:08<00:00,  9.00it/s]\n",
      "100%|██████████| 77/77 [00:11<00:00,  6.84it/s]\n",
      "100%|██████████| 77/77 [00:10<00:00,  7.44it/s]\n",
      "100%|██████████| 77/77 [00:11<00:00,  6.82it/s]\n",
      "100%|██████████| 77/77 [00:12<00:00,  6.23it/s]\n",
      "100%|██████████| 77/77 [00:08<00:00,  9.08it/s]\n",
      "100%|██████████| 77/77 [00:08<00:00,  9.16it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00, 10.95it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00,  9.98it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00, 10.50it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00, 10.09it/s]\n",
      "100%|██████████| 77/77 [00:08<00:00,  9.50it/s]\n",
      "100%|██████████| 77/77 [00:06<00:00, 11.20it/s]\n",
      "100%|██████████| 77/77 [00:09<00:00,  8.40it/s]\n",
      "100%|██████████| 77/77 [00:08<00:00,  9.54it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00, 10.31it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00,  9.92it/s]\n",
      "100%|██████████| 77/77 [00:08<00:00,  8.97it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00,  9.79it/s]\n",
      "100%|██████████| 77/77 [00:08<00:00,  9.59it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00,  9.97it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00, 10.01it/s]\n",
      "100%|██████████| 77/77 [00:09<00:00,  8.54it/s]\n",
      "100%|██████████| 77/77 [00:08<00:00,  9.00it/s]\n",
      "100%|██████████| 76/76 [00:06<00:00, 10.94it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00, 10.65it/s]\n",
      "100%|██████████| 77/77 [00:08<00:00,  9.21it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00,  9.75it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00,  9.70it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00,  9.71it/s]\n",
      "100%|██████████| 77/77 [00:08<00:00,  9.17it/s]\n",
      "100%|██████████| 77/77 [00:08<00:00,  9.45it/s]\n",
      "100%|██████████| 77/77 [00:08<00:00,  9.07it/s]\n",
      "100%|██████████| 77/77 [00:08<00:00,  9.34it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00,  9.72it/s]\n",
      "100%|██████████| 77/77 [00:08<00:00,  8.61it/s]\n",
      "100%|██████████| 77/77 [00:06<00:00, 11.17it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00,  9.91it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00, 10.65it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00, 10.66it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00, 10.43it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00, 10.88it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00, 10.49it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00, 10.43it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00, 10.81it/s]\n",
      "100%|██████████| 77/77 [00:06<00:00, 11.19it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00,  9.66it/s]\n",
      "100%|██████████| 77/77 [00:08<00:00,  9.49it/s]\n",
      "100%|██████████| 77/77 [00:06<00:00, 11.07it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00, 10.95it/s]\n",
      "100%|██████████| 77/77 [00:08<00:00,  9.51it/s]\n",
      "100%|██████████| 77/77 [00:08<00:00,  9.39it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00, 10.63it/s]\n",
      "100%|██████████| 77/77 [00:08<00:00,  8.78it/s]\n",
      "100%|██████████| 77/77 [00:09<00:00,  8.38it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00, 10.70it/s]\n",
      "0it [00:00, ?it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00, 10.95it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00, 10.19it/s]\n",
      "100%|██████████| 77/77 [00:08<00:00,  8.88it/s]\n",
      "100%|██████████| 77/77 [00:06<00:00, 11.31it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00, 10.76it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00,  9.71it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00,  9.63it/s]\n",
      "100%|██████████| 77/77 [00:08<00:00,  8.94it/s]\n",
      "100%|██████████| 77/77 [00:06<00:00, 11.18it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00, 10.95it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00, 10.87it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00, 10.24it/s]\n",
      "100%|██████████| 77/77 [00:06<00:00, 11.09it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00,  9.80it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00,  9.86it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00,  9.93it/s]\n",
      "100%|██████████| 77/77 [00:08<00:00,  9.04it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00, 10.00it/s]\n",
      "100%|██████████| 77/77 [00:06<00:00, 11.05it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00,  9.73it/s]\n",
      "100%|██████████| 77/77 [00:06<00:00, 11.23it/s]\n",
      "100%|██████████| 77/77 [00:08<00:00,  9.28it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00, 10.37it/s]\n",
      "100%|██████████| 77/77 [00:06<00:00, 11.13it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00,  9.82it/s]\n",
      "100%|██████████| 77/77 [00:09<00:00,  8.44it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00, 10.63it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00, 10.52it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00, 10.41it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00, 10.02it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00,  9.84it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00,  9.89it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00, 10.76it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00,  9.66it/s]\n",
      "100%|██████████| 77/77 [00:08<00:00,  9.45it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00, 10.13it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00, 10.11it/s]\n",
      "100%|██████████| 77/77 [00:06<00:00, 11.12it/s]\n",
      "100%|██████████| 77/77 [00:06<00:00, 12.37it/s]\n",
      "100%|██████████| 77/77 [00:08<00:00,  8.64it/s]\n",
      "100%|██████████| 77/77 [00:08<00:00,  9.62it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00, 10.04it/s]\n",
      "100%|██████████| 77/77 [00:06<00:00, 11.32it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00, 10.79it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00,  9.78it/s]\n",
      "100%|██████████| 77/77 [00:06<00:00, 11.01it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00, 10.64it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00, 10.42it/s]\n",
      "100%|██████████| 77/77 [00:08<00:00,  9.09it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00, 10.94it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00, 10.32it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00, 10.35it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00, 10.49it/s]\n",
      "100%|██████████| 77/77 [00:06<00:00, 11.76it/s]\n",
      "100%|██████████| 77/77 [00:06<00:00, 11.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting features from test data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00, 10.99it/s]\n",
      "100%|██████████| 77/77 [00:08<00:00,  9.46it/s]\n",
      "100%|██████████| 77/77 [00:08<00:00,  9.29it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00, 10.67it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00, 10.13it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00, 10.74it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00, 10.40it/s]\n",
      "100%|██████████| 77/77 [00:08<00:00,  9.59it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00, 10.68it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00, 10.46it/s]\n",
      "100%|██████████| 77/77 [00:08<00:00,  9.15it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00, 10.61it/s]\n",
      "100%|██████████| 77/77 [00:08<00:00,  8.66it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00, 10.01it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00,  9.68it/s]\n",
      "100%|██████████| 77/77 [00:08<00:00,  8.84it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00, 10.76it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00, 10.38it/s]\n",
      "100%|██████████| 77/77 [00:06<00:00, 11.21it/s]\n",
      "100%|██████████| 77/77 [00:08<00:00,  9.45it/s]\n",
      "100%|██████████| 77/77 [00:08<00:00,  9.10it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00, 10.61it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00, 10.24it/s]\n",
      "100%|██████████| 77/77 [00:08<00:00,  8.89it/s]\n",
      "100%|██████████| 77/77 [00:08<00:00,  9.38it/s]\n",
      "100%|██████████| 77/77 [00:08<00:00,  9.27it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00, 10.62it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00, 10.49it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00, 10.25it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00,  9.65it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00, 10.63it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00,  9.79it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00,  9.87it/s]\n",
      "100%|██████████| 77/77 [00:08<00:00,  9.37it/s]\n",
      "100%|██████████| 77/77 [00:08<00:00,  9.29it/s]\n",
      "100%|██████████| 77/77 [00:06<00:00, 11.55it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00, 10.48it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00, 10.24it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00,  9.91it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00, 10.48it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00,  9.67it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00, 10.64it/s]\n",
      "100%|██████████| 77/77 [00:08<00:00,  8.87it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00, 10.11it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00,  9.64it/s]\n",
      "100%|██████████| 77/77 [00:06<00:00, 11.13it/s]\n",
      "100%|██████████| 77/77 [00:08<00:00,  9.05it/s]\n",
      "100%|██████████| 77/77 [00:08<00:00,  9.53it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00,  9.77it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00, 10.05it/s]\n",
      "100%|██████████| 77/77 [00:08<00:00,  8.86it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00, 10.14it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00, 10.00it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00,  9.95it/s]\n",
      "100%|██████████| 77/77 [00:09<00:00,  8.54it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00, 10.20it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00, 10.27it/s]\n",
      "0it [00:00, ?it/s]\n",
      "100%|██████████| 77/77 [00:06<00:00, 11.19it/s]\n",
      "100%|██████████| 77/77 [00:08<00:00,  8.88it/s]\n",
      "100%|██████████| 77/77 [00:08<00:00,  9.26it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00,  9.92it/s]\n",
      "100%|██████████| 77/77 [00:08<00:00,  9.17it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00, 10.25it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00,  9.69it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00, 10.52it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00, 10.73it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00,  9.77it/s]\n",
      "100%|██████████| 77/77 [00:08<00:00,  8.97it/s]\n",
      "100%|██████████| 77/77 [00:08<00:00,  8.63it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00, 10.03it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00, 10.08it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00,  9.73it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00, 10.59it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00,  9.67it/s]\n",
      "100%|██████████| 77/77 [00:09<00:00,  8.20it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00, 10.64it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00,  9.77it/s]\n",
      "100%|██████████| 77/77 [00:06<00:00, 11.23it/s]\n",
      "100%|██████████| 77/77 [00:06<00:00, 11.77it/s]\n",
      "100%|██████████| 77/77 [00:09<00:00,  8.47it/s]\n",
      "100%|██████████| 77/77 [00:06<00:00, 12.11it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00,  9.66it/s]\n",
      "100%|██████████| 76/76 [00:06<00:00, 11.28it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00, 10.54it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00,  9.63it/s]\n",
      "100%|██████████| 77/77 [00:08<00:00,  9.31it/s]\n",
      "100%|██████████| 77/77 [00:08<00:00,  9.55it/s]\n",
      "100%|██████████| 77/77 [00:08<00:00,  9.19it/s]\n",
      "100%|██████████| 77/77 [00:06<00:00, 11.61it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00, 10.23it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00, 10.83it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00,  9.74it/s]\n",
      "100%|██████████| 77/77 [00:08<00:00,  9.03it/s]\n",
      "100%|██████████| 77/77 [00:06<00:00, 11.41it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00, 10.35it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00,  9.72it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00,  9.79it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00,  9.84it/s]\n",
      "100%|██████████| 77/77 [00:08<00:00,  9.52it/s]\n",
      "100%|██████████| 77/77 [00:08<00:00,  8.84it/s]\n",
      "100%|██████████| 77/77 [00:08<00:00,  9.25it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00, 10.60it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00, 10.52it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00, 10.69it/s]\n",
      "100%|██████████| 77/77 [00:08<00:00,  9.53it/s]\n",
      "100%|██████████| 77/77 [00:08<00:00,  8.97it/s]\n",
      "100%|██████████| 77/77 [00:08<00:00,  9.17it/s]\n",
      "100%|██████████| 77/77 [00:06<00:00, 11.19it/s]\n",
      "100%|██████████| 77/77 [00:08<00:00,  8.97it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00, 10.44it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00,  9.86it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00, 10.59it/s]\n",
      "100%|██████████| 77/77 [00:07<00:00,  9.72it/s]\n"
     ]
    }
   ],
   "source": [
    "print(\"Extraction features from train data\")\n",
    "trainData = feature_extraction('../tidigits/disc_4.1.1/tidigits/train')\n",
    "# Save the data to avoid computing it again\n",
    "np.savez('trainData.npz', trainData=trainData)\n",
    "\n",
    "print(\"Extracting features from test data\")\n",
    "testData = feature_extraction('../tidigits/disc_4.2.1/tidigits/test')\n",
    "np.savez('testData.npz', testData=testData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 Training and Validation Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitDataByGender(data_dict,gender, train_utterances):\n",
    "  train_data, val_data = [], []\n",
    "  for current_speaker in data_dict[gender].keys():\n",
    "    # if train_data contains 90 or more procent of total gender utterances, remaining data is stored as validation\n",
    "    if len(train_data) >= train_utterances:\n",
    "      #print(f\"len(train_data): {len(train_data)} > {train_utterances} --> Creating val set instead\")\n",
    "      val_data.extend(data_dict[gender][str(current_speaker)])\n",
    "    # Otherwise, we keep adding to train data until we achieve 90%\n",
    "    else:\n",
    "      #print(f\"len(train_data): {len(train_data)} < {train_utterances}\")\n",
    "      train_data.extend(data_dict[gender][str(current_speaker)])\n",
    "  print(f\"train_data: {len(train_data)} \\t val_data: {len(val_data)}\")\n",
    "  return train_data, val_data\n",
    "\n",
    "def splitData(total_data, split=0.1):\n",
    "  data_by_gender = {\"man\":{}, \"woman\": {}}\n",
    "  print(f\"Total_data length = {len(total_data)}\")\n",
    "  for data in total_data:\n",
    "    gender, speakerID, _, _ = tools3.path2info(data[\"filename\"])  # path2info returns tuple (gender, speakerID, digits, repetition)\n",
    "    if speakerID not in data_by_gender[gender]:\n",
    "      data_by_gender[gender][speakerID] = []\n",
    "    data_by_gender[gender][speakerID].append(data)\n",
    "\n",
    "  # Calculate total utterances by summing the lengths of each gender's list\n",
    "  total_male_utterances = sum(len(utterances) for utterances in data_by_gender[\"man\"].values())\n",
    "  total_female_utterances = sum(len(utterances) for utterances in data_by_gender[\"woman\"].values())\n",
    "\n",
    "  train_male_utterances = int(total_male_utterances * (1-split))     # compute how many male utterances to achieve 90%\n",
    "  train_female_utterances = int(total_female_utterances * (1-split)) # compute how many female utterances to achieve 90%\n",
    "  print(f\"total male utterances: {total_male_utterances}\\ntrain_male_utterances: {train_male_utterances}\")\n",
    "  print(f\"total female utterances: {total_female_utterances}\\ntrain_female_utterances: {train_female_utterances}\")\n",
    "\n",
    "  male_train_data, male_val_data = splitDataByGender(data_by_gender, \"man\", train_male_utterances)\n",
    "  female_train_data, female_val_data = splitDataByGender(data_by_gender, \"woman\", train_female_utterances)\n",
    "\n",
    "  train_data, val_data = [], []\n",
    "  train_data.extend(male_train_data)\n",
    "  train_data.extend(female_train_data)\n",
    "  val_data.extend(male_val_data)\n",
    "  val_data.extend(female_val_data)\n",
    "\n",
    "  print(f\"train data has {len(train_data)} elements\")\n",
    "  print(f\"val data has {len(val_data)} elements\")\n",
    "\n",
    "  return train_data, val_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainData has 8623 elements\n",
      "key: filename - ../tidigits/disc_4.1.1/tidigits/train/woman/cl/za.wav\n",
      "key: lmfcc - [[ 103.1708824   -82.31110635   14.29338738 ...  -38.65502294\n",
      "  -155.95559352  -48.36368006]\n",
      " [ 109.42118969  -11.18161627  117.72133428 ...    8.63922846\n",
      "   -40.50538129   21.4327149 ]\n",
      " [ 116.09423861  -46.88554472   25.04088065 ...  -75.15169037\n",
      "   -16.42606443   67.31586026]\n",
      " ...\n",
      " [ 203.16342222  109.37593821   16.40208643 ...  -27.98419695\n",
      "    -7.0104208     4.57830005]\n",
      " [ 164.15952204   87.7355812    85.18308825 ...  117.86320303\n",
      "    15.88054796   27.40947736]\n",
      " [ 170.64996493  107.48479268  124.78120348 ...   -5.24549154\n",
      "    49.41318624   10.30882344]]\n",
      "key: mspec - [[-0.12084544  1.03020631  0.46282056 ...  2.09551978  2.27765554\n",
      "   2.41654246]\n",
      " [ 2.54187435  3.06084006  3.07138977 ...  2.16677831  1.97251175\n",
      "   2.2343435 ]\n",
      " [ 1.52510378  2.79239405  1.89996556 ...  1.91766317  1.89914442\n",
      "   2.66109756]\n",
      " ...\n",
      " [ 4.40261508  3.47242441  2.87425463 ...  1.88805656  1.82493374\n",
      "   1.75817315]\n",
      " [ 5.22065891  3.47026996  1.56461146 ...  1.61069173  1.81613402\n",
      "   1.75268973]\n",
      " [ 4.78665577  4.24782695  3.10876783 ...  2.07996067  2.32817091\n",
      "   2.87288276]]\n",
      "key: targets - [39 39 39 39 39 39 39 39 39 39 39 39 39 39 39 39 39 39 39 39 39 39 39 39\n",
      " 39 39 39 39 39 39 39 40 40 40 40 40 40 40 40 41 41 41 41 41 58 58 58 58\n",
      " 58 58 58 59 59 59 59 60 60 21 21 21 21 21 21 21 21 21 22 23 33 34 34 34\n",
      " 34 34 34 34 34 34 34 34 34 34 34 34 34 34 35 30 30 30 30 30 30 30 30 31\n",
      " 31 31 31 32 32 32 32 32 32 32 39 39 39 39 39 39 39 39 39 39 39 39 39 39\n",
      " 39 39 39 39 39 39 39 39 39 39 40 41]\n"
     ]
    }
   ],
   "source": [
    "train_data_path = \"./trainData.npz\"\n",
    "test_data_path = \"./testData.npz\"\n",
    "trainData = np.load(train_data_path, allow_pickle=True)[\"trainData\"]\n",
    "testData = np.load(test_data_path, allow_pickle=True)[\"testData\"]\n",
    "\n",
    "print(f\"trainData has {len(trainData)} elements\")\n",
    "for key in trainData[0].keys():\n",
    "  print(f\"key: {key} - {trainData[0][key]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5 Acoustic Context (Dynamic Features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_context(features, context=3):\n",
    "    \"\"\"\n",
    "    Augments the features by adding context frames around each time step in the feature matrix.\n",
    "\n",
    "    Args:\n",
    "    features (np.array): The original feature matrix where each row is a time step and columns are features.\n",
    "    context (int): The number of frames to include from before and after the current frame.\n",
    "\n",
    "    Returns:\n",
    "    np.array: An augmented feature matrix including context.\n",
    "    \"\"\"\n",
    "    rows, cols = features.shape\n",
    "    context_features = np.zeros((rows, cols * (2 * context + 1)))\n",
    "\n",
    "    for i in range(rows):\n",
    "        for j in range(-context, context + 1):\n",
    "            if 0 <= i + j < rows:\n",
    "                context_features[i, (j + context) * cols: (j + context + 1) * cols] = features[i + j]\n",
    "            else:\n",
    "                # Use mirroring for edge cases\n",
    "                mirrored_index = min(max(0, i + j), rows - 1)\n",
    "                context_features[i, (j + context) * cols: (j + context + 1) * cols] = features[mirrored_index]\n",
    "\n",
    "    return context_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[103.1708824 , -82.31110635,  14.29338738, ...,   5.21621609,\n",
       "        -62.65111028, -48.97697333],\n",
       "       [103.1708824 , -82.31110635,  14.29338738, ..., -95.22756163,\n",
       "         16.2071291 ,  40.90530715],\n",
       "       [103.1708824 , -82.31110635,  14.29338738, ..., -72.15412933,\n",
       "        -66.9767564 , -71.81263328],\n",
       "       ...,\n",
       "       [153.06616189,  26.72406375,  29.80793612, ...,  -5.24549154,\n",
       "         49.41318624,  10.30882344],\n",
       "       [128.26044834,  37.93693872,  99.15316669, ...,  -5.24549154,\n",
       "         49.41318624,  10.30882344],\n",
       "       [177.653294  ,  91.29562855,  43.55066378, ...,  -5.24549154,\n",
       "         49.41318624,  10.30882344]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add_context(trainData[0][\"lmfcc\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total_data length = 8623\n",
      "total male utterances: 4235\n",
      "train_male_utterances: 3811\n",
      "total female utterances: 4388\n",
      "train_female_utterances: 3949\n",
      "train_data: 3850 \t val_data: 385\n",
      "train_data: 4003 \t val_data: 385\n",
      "train data has 7853 elements\n",
      "val data has 770 elements\n"
     ]
    }
   ],
   "source": [
    "trainData, valData = splitData(trainData)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data:\n",
      "N: 1366280\n",
      "D_lmfcc: 13 \tD_mspec: 40\n",
      "Validation data:\n",
      "N: 140777\n",
      "D_lmfcc: 13 \tD_mspec: 40\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "print(\"Training data:\")\n",
    "N = sum([len(data['targets']) for data in trainData])\n",
    "D_lmfcc = trainData[0][\"lmfcc\"].shape[1]\n",
    "D_mspec = trainData[0][\"mspec\"].shape[1]\n",
    "print(f\"N: {N}\")\n",
    "print(f\"D_lmfcc: {D_lmfcc} \\tD_mspec: {D_mspec}\")\n",
    "\n",
    "# Val\n",
    "print(\"Validation data:\")\n",
    "N = sum([len(data['targets']) for data in valData])\n",
    "D_lmfcc = valData[0][\"lmfcc\"].shape[1]\n",
    "D_mspec = valData[0][\"mspec\"].shape[1]\n",
    "print(f\"N: {N}\")\n",
    "print(f\"D_lmfcc: {D_lmfcc} \\tD_mspec: {D_mspec}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.6 Feature Standardisation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preproccsed all data\n",
      "trainLMFCCX: (1366280, 91) \t trainMSPECX: (1366280, 40) \t trainY: torch.Size([1366280, 61])\n",
      "valLMFCCX: (140777, 91) \t valMSPECX: (140777, 40) \t valY: torch.Size([140777, 61])\n",
      "testLMFCCX: (1526682, 91) \t testMSPECX: (1526682, 40) \t testY: torch.Size([1526682, 61])\n"
     ]
    }
   ],
   "source": [
    "def preprocessing(totalData, _num_classes, context=True):\n",
    "    Ns = [len(data['targets']) for data in totalData]\n",
    "    N = sum(Ns)\n",
    "    D_lmfcc = totalData[0][\"lmfcc\"].shape[1] * (7 if context else 1)\n",
    "    D_mspec = totalData[0][\"mspec\"].shape[1]\n",
    "    X_lmfcc = np.zeros((N, D_lmfcc)).astype(\"float32\")\n",
    "    X_mspec = np.zeros((N, D_mspec)).astype(\"float32\")\n",
    "    Y = np.zeros(N, dtype=int)\n",
    "\n",
    "    startPointer = 0\n",
    "    for i, data in enumerate(totalData):\n",
    "        lmfcc = data[\"lmfcc\"]\n",
    "        if context:\n",
    "            lmfcc = add_context(lmfcc, context=3)\n",
    "        X_lmfcc[startPointer: startPointer + Ns[i]] = lmfcc\n",
    "        X_mspec[startPointer: startPointer + Ns[i]] = data[\"mspec\"]\n",
    "        Y[startPointer: startPointer + Ns[i]] = data[\"targets\"]\n",
    "        startPointer += Ns[i]\n",
    "\n",
    "    Y = F.one_hot(torch.tensor(Y), num_classes=_num_classes)\n",
    "    return X_lmfcc, X_mspec, Y\n",
    "\n",
    "# Creating scalers to standardize the data\n",
    "scalerLMFCC = StandardScaler()\n",
    "scalerMSPEC = StandardScaler()\n",
    "\n",
    "# Preprocessing the data\n",
    "trainLMFCCX, trainMSPECX, trainY = preprocessing(trainData, len(stateList))\n",
    "valLMFCCX, valMSPECX, valY = preprocessing(valData, len(stateList))\n",
    "testLMFCCX, testMSPECX, testY = preprocessing(testData, len(stateList))\n",
    "\n",
    "# Standardizing the data\n",
    "scalerLMFCC.fit(trainLMFCCX)\n",
    "trainLMFCCX = scalerLMFCC.transform(trainLMFCCX)\n",
    "valLMFCCX = scalerLMFCC.transform(valLMFCCX)\n",
    "testLMFCCX = scalerLMFCC.transform(testLMFCCX)\n",
    "\n",
    "scalerMSPEC.fit(trainMSPECX)\n",
    "trainMSPECX = scalerMSPEC.transform(trainMSPECX)\n",
    "valMSPECX = scalerMSPEC.transform(valMSPECX)\n",
    "testMSPECX = scalerMSPEC.transform(testMSPECX)\n",
    "\n",
    "print(f\"Preproccsed all data\")\n",
    "print(f\"trainLMFCCX: {trainLMFCCX.shape} \\t trainMSPECX: {trainMSPECX.shape} \\t trainY: {trainY.shape}\")\n",
    "print(f\"valLMFCCX: {valLMFCCX.shape} \\t valMSPECX: {valMSPECX.shape} \\t valY: {valY.shape}\")\n",
    "print(f\"testLMFCCX: {testLMFCCX.shape} \\t testMSPECX: {testMSPECX.shape} \\t testY: {testY.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 Phoneme Recognition with Deep Neural Networks\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FC_ReLU(in_dim: int,\n",
    "            out_dim: int) -> nn.Sequential:\n",
    "  \"\"\"\n",
    "    Creates a fully connected layer with ReLU activation.\n",
    "\n",
    "    Parameters:\n",
    "    - in_dim (int): The number of input features.\n",
    "    - out_dim (int): The number of output features.\n",
    "\n",
    "    Returns:\n",
    "    - nn.Sequential: A sequential container with a linear layer, batch normalization, and ReLU activation.\n",
    "  \"\"\"\n",
    "  return nn.Sequential(\n",
    "      nn.Linear(in_features=in_dim, out_features=out_dim),\n",
    "      nn.BatchNorm1d(out_dim),\n",
    "      nn.ReLU()\n",
    "  )\n",
    "\n",
    "def FC_Sigmoid(in_dim: int,\n",
    "               out_dim: int) -> nn.Sequential:\n",
    "  \"\"\"\n",
    "    Creates a fully connected layer with Sigmoid activation.\n",
    "\n",
    "    Parameters:\n",
    "    - in_dim (int): The number of input features.\n",
    "    - out_dim (int): The number of output features.\n",
    "\n",
    "    Returns:\n",
    "    - nn.Sequential: A sequential container with a linear layer, batch normalization, and Sigmoid activation.\n",
    "  \"\"\"\n",
    "  return nn.Sequential(\n",
    "      nn.Linear(in_features=in_dim, out_features=out_dim),\n",
    "      nn.BatchNorm1d(out_dim),\n",
    "      nn.Sigmoid()\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Net(\n",
      "  (network): Sequential(\n",
      "    (0): Sequential(\n",
      "      (0): Linear(in_features=91, out_features=256, bias=True)\n",
      "      (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU()\n",
      "    )\n",
      "    (1): Sequential(\n",
      "      (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "      (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU()\n",
      "    )\n",
      "    (2): Sequential(\n",
      "      (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "      (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU()\n",
      "    )\n",
      "    (3): Linear(in_features=256, out_features=61, bias=True)\n",
      "  )\n",
      ")\n",
      "number of prameters: 172349\n"
     ]
    },
    {
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied: '/content'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "Input \u001b[0;32mIn [27]\u001b[0m, in \u001b[0;36m<cell line: 117>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    115\u001b[0m model_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_epochs_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbatch_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_batch_size_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhidden_dims\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_hidden_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcurrent_time\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    116\u001b[0m model_subfolder \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(base_path, model_id)\n\u001b[0;32m--> 117\u001b[0m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmakedirs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_subfolder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexist_ok\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;66;03m# Define the paths for saving the model and logs\u001b[39;00m\n\u001b[1;32m    120\u001b[0m model_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(model_subfolder, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_dict.pt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/os.py:215\u001b[0m, in \u001b[0;36mmakedirs\u001b[0;34m(name, mode, exist_ok)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m head \u001b[38;5;129;01mand\u001b[39;00m tail \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m path\u001b[38;5;241m.\u001b[39mexists(head):\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 215\u001b[0m         \u001b[43mmakedirs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhead\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexist_ok\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexist_ok\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mFileExistsError\u001b[39;00m:\n\u001b[1;32m    217\u001b[0m         \u001b[38;5;66;03m# Defeats race condition when another thread created the path\u001b[39;00m\n\u001b[1;32m    218\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/os.py:215\u001b[0m, in \u001b[0;36mmakedirs\u001b[0;34m(name, mode, exist_ok)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m head \u001b[38;5;129;01mand\u001b[39;00m tail \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m path\u001b[38;5;241m.\u001b[39mexists(head):\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 215\u001b[0m         \u001b[43mmakedirs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhead\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexist_ok\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexist_ok\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mFileExistsError\u001b[39;00m:\n\u001b[1;32m    217\u001b[0m         \u001b[38;5;66;03m# Defeats race condition when another thread created the path\u001b[39;00m\n\u001b[1;32m    218\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "    \u001b[0;31m[... skipping similar frames: makedirs at line 215 (4 times)]\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/os.py:215\u001b[0m, in \u001b[0;36mmakedirs\u001b[0;34m(name, mode, exist_ok)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m head \u001b[38;5;129;01mand\u001b[39;00m tail \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m path\u001b[38;5;241m.\u001b[39mexists(head):\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 215\u001b[0m         \u001b[43mmakedirs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhead\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexist_ok\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexist_ok\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mFileExistsError\u001b[39;00m:\n\u001b[1;32m    217\u001b[0m         \u001b[38;5;66;03m# Defeats race condition when another thread created the path\u001b[39;00m\n\u001b[1;32m    218\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/os.py:225\u001b[0m, in \u001b[0;36mmakedirs\u001b[0;34m(name, mode, exist_ok)\u001b[0m\n\u001b[1;32m    223\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    224\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 225\u001b[0m     \u001b[43mmkdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    226\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n\u001b[1;32m    227\u001b[0m     \u001b[38;5;66;03m# Cannot rely on checking for EEXIST, since the operating system\u001b[39;00m\n\u001b[1;32m    228\u001b[0m     \u001b[38;5;66;03m# could give priority to other errors like EACCES or EROFS\u001b[39;00m\n\u001b[1;32m    229\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m exist_ok \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m path\u001b[38;5;241m.\u001b[39misdir(name):\n",
      "\u001b[0;31mPermissionError\u001b[0m: [Errno 13] Permission denied: '/content'"
     ]
    }
   ],
   "source": [
    "# define the neural network architecture\n",
    "class Net(torch.nn.Module):\n",
    "  def __init__(self,\n",
    "               input_dim: int,\n",
    "               hidden_dims: list[int],\n",
    "               output_dim: int,\n",
    "               active_type: str =\"ReLU\"):\n",
    "    super().__init__()\n",
    "    self.input_dim = input_dim\n",
    "    self.hidden_dims = hidden_dims\n",
    "    self.output_dim = output_dim\n",
    "    self.active_type = active_type\n",
    "\n",
    "    self.create_network()\n",
    "    self.weight_init()\n",
    "\n",
    "  def create_network(self) -> None:\n",
    "    \"\"\"\n",
    "      Constructs the neural network architecture.\n",
    "\n",
    "      This method creates a sequential container with fully connected layers and the specified activation functions.\n",
    "    \"\"\"\n",
    "    modules = []\n",
    "    input_dim = self.input_dim\n",
    "    if self.hidden_dims: # Check if hidden_dims is not empty\n",
    "      for hidden_dim in self.hidden_dims:\n",
    "        if self.active_type == \"ReLU\":\n",
    "          modules.append(FC_ReLU(input_dim, hidden_dim))\n",
    "        elif self.active_type == \"Sigmoid\":\n",
    "          modules.append(FC_Sigmoid(input_dim, hidden_dim))\n",
    "\n",
    "        input_dim = hidden_dim\n",
    "\n",
    "    modules.append(nn.Linear(in_features=input_dim, out_features=self.output_dim))\n",
    "\n",
    "    self.network = nn.Sequential(*modules)\n",
    "    self.layers = len(self.hidden_dims) + 1\n",
    "\n",
    "  def weight_init(self) -> None:\n",
    "    \"\"\"\n",
    "      Initializes the weights of the network.\n",
    "\n",
    "      This method applies He initialization for ReLU activations and Xavier initialization for Sigmoid activations.\n",
    "    \"\"\"\n",
    "    for m in self.modules():\n",
    "        if isinstance(m, (nn.Linear)):\n",
    "          if self.active_type == \"ReLU\":\n",
    "            # He initialization\n",
    "            nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='relu')\n",
    "          elif self.active_type == \"Sigmoid\":\n",
    "            # Xavier initalization\n",
    "            nn.init.xavier_uniform_(m.weight)\n",
    "\n",
    "  def forward(self,\n",
    "              X: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "      Forward pass through the network.\n",
    "\n",
    "      Parameters:\n",
    "      - X (torch.Tensor): Input tensor.\n",
    "\n",
    "      Returns:\n",
    "      - torch.Tensor: Output tensor after applying softmax activation.\n",
    "    \"\"\"\n",
    "    output = self.network(X)\n",
    "    return F.softmax(output, dim=1)\n",
    "\n",
    "def count_parameters(net):\n",
    "    return sum(p.numel() for p in net.parameters() if p.requires_grad)\n",
    "\n",
    "featureType = \"lmfcc\"\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "if featureType == \"lmfcc\":\n",
    "  # prepare/load the data into tensors\n",
    "  train_x, val_x, test_x = trainLMFCCX, valLMFCCX, testLMFCCX\n",
    "else:\n",
    "  train_x, val_x, test_x = trainMSPECX, valMSPECX, testLMFCCX\n",
    "\n",
    "train_y, val_y, test_y = trainY.float(), valY.float(), testY.float()\n",
    "\n",
    "# instantiate the network and print the structure\n",
    "input_dim = train_x.shape[1]\n",
    "hidden_dims = [256,256,256]\n",
    "output_dim = len(stateList) # classes\n",
    "net = Net(input_dim, hidden_dims, output_dim).to(device)\n",
    "print(net)\n",
    "print('number of prameters:', count_parameters(net))\n",
    "\n",
    "# define your loss criterion (see https://pytorch.org/docs/stable/nn.html#loss-functions)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# define the optimizer\n",
    "optimizer = torch.optim.Adam(net.parameters())\n",
    "\n",
    "batch_size = 128    # they had 256 but said we could lower it to reduce compute\n",
    "\n",
    "# create the data loaders for training and validation sets\n",
    "train_dataset = torch.utils.data.TensorDataset(torch.Tensor(train_x), train_y)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_dataset = torch.utils.data.TensorDataset(torch.Tensor(val_x), val_y)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_dataset = torch.utils.data.TensorDataset(torch.Tensor(test_x), test_y)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# train the network\n",
    "num_epochs = 50   # default 100 but starting with lower to see if it works\n",
    "\n",
    "# Path to store everything\n",
    "base_path = \"/content/drive/MyDrive/KTH/DD2119_Speech_Recognition/Labs/models\"\n",
    "\n",
    "current_time = datetime.now().strftime(\"%H%M\")\n",
    "model_id = f\"model_{num_epochs}_epochs_{batch_size}_batch_size_{hidden_dims}_hidden_{current_time}\"\n",
    "model_subfolder = os.path.join(base_path, model_id)\n",
    "os.makedirs(model_subfolder, exist_ok=True)\n",
    "\n",
    "# Define the paths for saving the model and logs\n",
    "model_path = os.path.join(model_subfolder, f'model_dict.pt')\n",
    "logs_path = os.path.join(model_subfolder, 'logs')\n",
    "\n",
    "# setup logging so that you can follow training using TensorBoard (see https://pytorch.org/docs/stable/tensorboard.html)\n",
    "writer = SummaryWriter(log_dir = logs_path)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "  net.train()\n",
    "  train_loss, train_acc = 0.0, 0.0\n",
    "  i = 0\n",
    "  for inputs, labels in tqdm(train_loader, desc=\"Training Progress\", unit=\"batch\"):\n",
    "    inputs, labels = inputs.to(device), labels.to(device)\n",
    "    # zero the parameter gradients\n",
    "    optimizer.zero_grad()\n",
    "    # forward + backward + optimize\n",
    "    outputs = net(inputs)\n",
    "    loss = criterion(outputs, labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    # accumulate the training loss and acc\n",
    "    train_loss += loss.item()\n",
    "    _, predicted = torch.max(outputs, 1)    # extract the most probable label prediction\n",
    "    _, correct = torch.max(labels, 1)       # extract the most correct label prediction\n",
    "    train_acc += (predicted == correct).sum().item() # count the sum of all label predictions that were correct\n",
    "\n",
    "  net.eval()\n",
    "  with torch.no_grad():\n",
    "      val_loss, val_acc = 0.0, 0.0\n",
    "      for inputs, labels in tqdm(val_loader, desc=\"Validation Progress\", unit=\"batch\"):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        val_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs, 1)    # extract the most probable label prediction\n",
    "        _, correct = torch.max(labels, 1)       # extract the most correct label prediction\n",
    "        val_acc += (predicted == correct).sum().item() # count the sum of all label predictions that were correct\n",
    "\n",
    "\n",
    "  # print the epoch loss\n",
    "  train_loss /= len(train_loader)\n",
    "  val_loss /= len(val_loader)\n",
    "  train_acc /= len(train_loader)\n",
    "  val_acc /= len(val_loader)\n",
    "\n",
    "  print(f'Epoch {epoch}: train_loss={train_loss}, val_loss={val_loss}, train_acc={train_acc}, val_acc={val_acc}')\n",
    "  writer.add_scalars('loss',{'train':train_loss,'val':val_loss},epoch)\n",
    "  writer.add_scalars('acc', {'train':train_acc,'val':val_acc}, epoch)\n",
    "\n",
    "net.eval()\n",
    "with torch.no_grad():\n",
    "    test_loss, test_acc = 0.0, 0.0\n",
    "    for inputs, labels in tqdm(test_loader, desc=\"Test Progress\", unit=\"batch\"):\n",
    "      inputs, labels = inputs.to(device), labels.to(device)\n",
    "      outputs = net(inputs)\n",
    "      loss = criterion(outputs, labels)\n",
    "      test_loss += loss.item()\n",
    "      _, predicted = torch.max(outputs, 1)    # extract the most probable label prediction\n",
    "      _, correct = torch.max(labels, 1)       # extract the most correct label prediction\n",
    "      test_acc += (predicted == correct).sum().item() # count the sum of all label predictions that were correct\n",
    "\n",
    "test_loss /= len(test_loader)\n",
    "test_acc /= len(test_loader)\n",
    "\n",
    "print(f'Test Result: test_loss={test_loss} \\t test_acc={test_acc}')\n",
    "\n",
    "# save the trained network\n",
    "torch.save(net.state_dict(), model_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  5.1 Detailed Evaluation\n",
    "\n",
    "### 1. frame-by-frame at the state level : count the number of frames (time steps) that were correctly classified over the total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "10.2\n",
      "NVIDIA GeForce RTX 3060 Laptop GPU\n",
      "Using device: cpu\n",
      "Net(\n",
      "  (network): Sequential(\n",
      "    (0): Sequential(\n",
      "      (0): Linear(in_features=91, out_features=256, bias=True)\n",
      "      (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU()\n",
      "    )\n",
      "    (1): Sequential(\n",
      "      (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "      (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU()\n",
      "    )\n",
      "    (2): Sequential(\n",
      "      (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "      (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU()\n",
      "    )\n",
      "    (3): Linear(in_features=256, out_features=61, bias=True)\n",
      "  )\n",
      ")\n",
      "number of prameters: 172349\n"
     ]
    }
   ],
   "source": [
    "# PREPARE THE DATA \n",
    "\n",
    "featureType = \"lmfcc\"\n",
    "\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.version.cuda)\n",
    "print(torch.cuda.get_device_name(0))\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(\"cpu\")  # Switch to CPU to test the model setup\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "if featureType == \"lmfcc\":\n",
    "  # prepare/load the data into tensors\n",
    "  train_x, val_x, test_x = trainLMFCCX, valLMFCCX, testLMFCCX\n",
    "else:\n",
    "  train_x, val_x, test_x = trainMSPECX, valMSPECX, testLMFCCX\n",
    "\n",
    "train_y, val_y, test_y = trainY.float(), valY.float(), testY.float()\n",
    "\n",
    "# instantiate the network and print the structure\n",
    "input_dim = train_x.shape[1]\n",
    "hidden_dims = [256,256,256]\n",
    "output_dim = len(stateList) # classes\n",
    "net = Net(input_dim, hidden_dims, output_dim).to(device)\n",
    "print(net)\n",
    "print('number of prameters:', count_parameters(net))\n",
    "\n",
    "# define your loss criterion (see https://pytorch.org/docs/stable/nn.html#loss-functions)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# define the optimizer\n",
    "optimizer = torch.optim.Adam(net.parameters())\n",
    "\n",
    "batch_size = 256  # they had 256 but said we could lower it to reduce compute\n",
    "\n",
    "# create the data loaders for training and validation sets\n",
    "train_dataset = torch.utils.data.TensorDataset(torch.Tensor(train_x), train_y)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_dataset = torch.utils.data.TensorDataset(torch.Tensor(val_x), val_y)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_dataset = torch.utils.data.TensorDataset(torch.Tensor(test_x), test_y)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# train the network\n",
    "num_epochs = 50   # default 100 but starting with lower to see if it works\n",
    "\n",
    "# Path to store everything\n",
    "base_path = \"./Lab3/models\"\n",
    "\n",
    "current_time = datetime.now().strftime(\"%H%M\")\n",
    "model_id = f\"model_{num_epochs}_epochs_{batch_size}_batch_size_{hidden_dims}_hidden_{current_time}\"\n",
    "model_subfolder = os.path.join(base_path, model_id)\n",
    "os.makedirs(model_subfolder, exist_ok=True)\n",
    "\n",
    "# Define the paths for saving the model and logs\n",
    "model_path = os.path.join(model_subfolder, f'model_dict.pt')\n",
    "logs_path = os.path.join(model_subfolder, 'logs')\n",
    "\n",
    "# setup logging so that you can follow training using TensorBoard (see https://pytorch.org/docs/stable/tensorboard.html)\n",
    "writer = SummaryWriter(log_dir = logs_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Validation Loss: 3.531690835952759\n",
      "Epoch 1: Validation Loss: 3.501129150390625\n",
      "Epoch 2: Validation Loss: 3.4823482036590576\n",
      "Epoch 3: Validation Loss: 3.4678592681884766\n",
      "Epoch 4: Validation Loss: 3.463560104370117\n",
      "Epoch 5: Validation Loss: 3.4502956867218018\n",
      "Epoch 6: Validation Loss: 3.447597026824951\n",
      "Epoch 7: Validation Loss: 3.450857639312744\n",
      "Epoch 8: Validation Loss: 3.4508142471313477\n",
      "Epoch 9: Validation Loss: 3.4441487789154053\n",
      "Epoch 10: Validation Loss: 3.4435527324676514\n",
      "Epoch 11: Validation Loss: 3.4416310787200928\n",
      "Epoch 12: Validation Loss: 3.4454307556152344\n",
      "Epoch 13: Validation Loss: 3.4393093585968018\n",
      "Epoch 14: Validation Loss: 3.438217878341675\n",
      "Epoch 15: Validation Loss: 3.4393720626831055\n",
      "Epoch 16: Validation Loss: 3.4403274059295654\n",
      "Epoch 17: Validation Loss: 3.442626714706421\n",
      "Epoch 18: Validation Loss: 3.438491106033325\n",
      "Epoch 19: Validation Loss: 3.439075469970703\n",
      "Epoch 20: Validation Loss: 3.443674325942993\n",
      "Epoch 21: Validation Loss: 3.4371416568756104\n",
      "Epoch 22: Validation Loss: 3.432307720184326\n",
      "Epoch 23: Validation Loss: 3.4313924312591553\n",
      "Epoch 24: Validation Loss: 3.436553716659546\n",
      "Epoch 25: Validation Loss: 3.430309772491455\n",
      "Epoch 26: Validation Loss: 3.429506778717041\n",
      "Epoch 27: Validation Loss: 3.4299750328063965\n",
      "Epoch 28: Validation Loss: 3.4271936416625977\n",
      "Epoch 29: Validation Loss: 3.4298155307769775\n",
      "Epoch 30: Validation Loss: 3.4321823120117188\n",
      "Epoch 31: Validation Loss: 3.42930269241333\n",
      "Epoch 32: Validation Loss: 3.430732011795044\n",
      "Epoch 33: Validation Loss: 3.429438352584839\n",
      "Epoch 34: Validation Loss: 3.427828073501587\n",
      "Epoch 35: Validation Loss: 3.4320125579833984\n",
      "Epoch 36: Validation Loss: 3.4322500228881836\n",
      "Epoch 37: Validation Loss: 3.4275286197662354\n",
      "Epoch 38: Validation Loss: 3.4308552742004395\n",
      "Epoch 39: Validation Loss: 3.42983078956604\n",
      "Epoch 40: Validation Loss: 3.4253015518188477\n",
      "Epoch 41: Validation Loss: 3.429743528366089\n",
      "Epoch 42: Validation Loss: 3.428453207015991\n",
      "Epoch 43: Validation Loss: 3.424271583557129\n",
      "Epoch 44: Validation Loss: 3.4292490482330322\n",
      "Epoch 45: Validation Loss: 3.4291512966156006\n",
      "Epoch 46: Validation Loss: 3.425717353820801\n",
      "Epoch 47: Validation Loss: 3.429070472717285\n",
      "Epoch 48: Validation Loss: 3.4337406158447266\n",
      "Epoch 49: Validation Loss: 3.4237301349639893\n"
     ]
    }
   ],
   "source": [
    "# TRAINING THE MODEL \n",
    "# use 91 instead of 13 because we use context in the data check part 4.5 \n",
    "model = Net(input_dim=91, hidden_dims=[256, 256, 256], output_dim=output_dim, active_type=\"ReLU\")\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for inputs, targets in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_loss = sum(criterion(model(inputs), targets) for inputs, targets in val_loader) / len(val_loader)\n",
    "    print(f'Epoch {epoch}: Validation Loss: {val_loss.item()}')\n",
    "\n",
    "\n",
    "# save the trained network\n",
    "torch.save(model.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation Progress:   0%|          | 0/550 [00:00<?, ?batch/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation Progress: 100%|██████████| 550/550 [00:01<00:00, 352.44batch/s]\n"
     ]
    }
   ],
   "source": [
    "# EVALUATE THE MODEL\n",
    "\n",
    "# model.eval()\n",
    "# test_loss = 0\n",
    "# correct = 0\n",
    "# total_samples = 0  # To track the total number of samples processed\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     for inputs, targets in test_loader:\n",
    "#         outputs = model(inputs)\n",
    "#         test_loss += criterion(outputs, targets).item()\n",
    "\n",
    "#         # Get predictions and reshape targets for comparison\n",
    "#         pred = outputs.argmax(dim=1)  # This makes `pred` 1D, matching `targets`\n",
    "        \n",
    "#         # Compare predictions to targets\n",
    "#         correct += (pred == targets).sum().item()\n",
    "#         total_samples += targets.size(0)\n",
    "\n",
    "# test_loss /= len(test_loader)\n",
    "# accuracy = 100. * correct / total_samples\n",
    "# print(f'Test set: Average loss: {test_loss:.4f}, Accuracy: {correct}/{total_samples} ({accuracy:.0f}%)')\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    val_loss, val_acc = 0.0, 0.0\n",
    "    for inputs, labels in tqdm(val_loader, desc=\"Validation Progress\", unit=\"batch\"):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        val_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs, 1)    # extract the most probable label prediction\n",
    "        _, correct = torch.max(labels, 1)       # extract the most correct label prediction\n",
    "        val_acc += (predicted == correct).sum().item() # count the sum of all label predictions that were correct\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. frame-by-frame at the phoneme level : same as 1., but merge all states that correspond to the same phoneme, for example ox_0, ox_1 and ox_2 are merged to ox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def evaluate_model(model, loader):\n",
    "    model.eval()\n",
    "    all_outputs = []\n",
    "    all_targets = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in loader:\n",
    "            outputs = model(inputs)\n",
    "            all_outputs.append(outputs)\n",
    "            all_targets.append(targets)\n",
    "    return torch.cat(all_outputs), torch.cat(all_targets)\n",
    "\n",
    "\n",
    "outputs, targets = evaluate_model(model, test_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 864x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_posteriors(outputs, targets, idx):\n",
    "    \"\"\"\n",
    "    Plot the posterior probabilities for each phoneme class for a single example.\n",
    "    \n",
    "    Args:\n",
    "    outputs (torch.Tensor): The output from the model, typically log probabilities.\n",
    "    targets (torch.Tensor): The actual targets (labels).\n",
    "    idx (int): Index of the utterance to plot.\n",
    "    \"\"\"\n",
    "    # Convert outputs to probabilities\n",
    "    probabilities = torch.exp(outputs[idx])  # Assuming output is log_softmax\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.plot(probabilities.cpu().numpy(), label='Posterior probabilities')\n",
    "    plt.scatter([i for i in range(len(probabilities))], targets[idx].cpu().numpy(), color='red', label='True Labels', marker='x')  # Assuming target is one-hot or indices\n",
    "    plt.title('Posterior Probabilities vs. True Labels')\n",
    "    plt.xlabel('Time Steps')\n",
    "    plt.ylabel('Probabilities')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Example usage: plot for the first test example\n",
    "plot_posteriors(outputs, targets, 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (1526682) must match the size of tensor b (61) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [36]\u001b[0m, in \u001b[0;36m<cell line: 24>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m accuracy\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# Example of usage\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m accuracy \u001b[38;5;241m=\u001b[39m \u001b[43mframe_by_frame_accuracy\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFrame-by-frame accuracy at state level: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maccuracy \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m100\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Input \u001b[0;32mIn [36]\u001b[0m, in \u001b[0;36mframe_by_frame_accuracy\u001b[0;34m(outputs, targets)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Assuming outputs are raw logits, apply softmax to convert to probabilities\u001b[39;00m\n\u001b[1;32m     17\u001b[0m predicted_states \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margmax(torch\u001b[38;5;241m.\u001b[39msoftmax(outputs, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 18\u001b[0m correct \u001b[38;5;241m=\u001b[39m (\u001b[43mpredicted_states\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m)\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39msum()\n\u001b[1;32m     19\u001b[0m total \u001b[38;5;241m=\u001b[39m targets\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     20\u001b[0m accuracy \u001b[38;5;241m=\u001b[39m correct \u001b[38;5;241m/\u001b[39m total\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (1526682) must match the size of tensor b (61) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def frame_by_frame_accuracy(outputs, targets):\n",
    "    \"\"\"\n",
    "    Compute the frame-by-frame accuracy for state and phoneme levels.\n",
    "    \n",
    "    Args:\n",
    "    outputs (torch.Tensor): Logits from the model.\n",
    "    targets (torch.Tensor): True labels.\n",
    "\n",
    "    Returns:\n",
    "    float: Accuracy at the state level.\n",
    "    \"\"\"\n",
    "    # Assuming outputs are raw logits, apply softmax to convert to probabilities\n",
    "    predicted_states = torch.argmax(torch.softmax(outputs, dim=1), dim=1)\n",
    "    correct = (predicted_states == targets).float().sum()\n",
    "    total = targets.shape[0]\n",
    "    accuracy = correct / total\n",
    "    return accuracy.item()\n",
    "\n",
    "# Example of usage\n",
    "accuracy = frame_by_frame_accuracy(outputs, targets)\n",
    "print(f\"Frame-by-frame accuracy at state level: {accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Classification metrics can't handle a mix of multilabel-indicator and multiclass targets",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [55]\u001b[0m, in \u001b[0;36m<cell line: 18>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m conf_matrix\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Example of usage\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m conf_matrix \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_confusion_matrix\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m91\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConfusion Matrix:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, conf_matrix)\n",
      "Input \u001b[0;32mIn [55]\u001b[0m, in \u001b[0;36mcompute_confusion_matrix\u001b[0;34m(outputs, targets, num_classes)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03mCompute confusion matrix for evaluating classification performance.\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;03m\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;124;03mnp.ndarray: A confusion matrix.\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     13\u001b[0m preds \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margmax(torch\u001b[38;5;241m.\u001b[39msoftmax(outputs, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 14\u001b[0m conf_matrix \u001b[38;5;241m=\u001b[39m \u001b[43mconfusion_matrix\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtargets\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m conf_matrix\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:307\u001b[0m, in \u001b[0;36mconfusion_matrix\u001b[0;34m(y_true, y_pred, labels, sample_weight, normalize)\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconfusion_matrix\u001b[39m(\n\u001b[1;32m    223\u001b[0m     y_true, y_pred, \u001b[38;5;241m*\u001b[39m, labels\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, normalize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    224\u001b[0m ):\n\u001b[1;32m    225\u001b[0m     \u001b[38;5;124;03m\"\"\"Compute confusion matrix to evaluate the accuracy of a classification.\u001b[39;00m\n\u001b[1;32m    226\u001b[0m \n\u001b[1;32m    227\u001b[0m \u001b[38;5;124;03m    By definition a confusion matrix :math:`C` is such that :math:`C_{i, j}`\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[38;5;124;03m    (0, 2, 1, 1)\u001b[39;00m\n\u001b[1;32m    306\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 307\u001b[0m     y_type, y_true, y_pred \u001b[38;5;241m=\u001b[39m \u001b[43m_check_targets\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    308\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m y_type \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmulticlass\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    309\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m is not supported\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m y_type)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:93\u001b[0m, in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     90\u001b[0m     y_type \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmulticlass\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(y_type) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m---> 93\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     94\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mClassification metrics can\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt handle a mix of \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m targets\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m     95\u001b[0m             type_true, type_pred\n\u001b[1;32m     96\u001b[0m         )\n\u001b[1;32m     97\u001b[0m     )\n\u001b[1;32m     99\u001b[0m \u001b[38;5;66;03m# We can't have more than one value on y_type => The set is no more needed\u001b[39;00m\n\u001b[1;32m    100\u001b[0m y_type \u001b[38;5;241m=\u001b[39m y_type\u001b[38;5;241m.\u001b[39mpop()\n",
      "\u001b[0;31mValueError\u001b[0m: Classification metrics can't handle a mix of multilabel-indicator and multiclass targets"
     ]
    }
   ],
   "source": [
    "def compute_confusion_matrix(outputs, targets, num_classes):\n",
    "    \"\"\"\n",
    "    Compute confusion matrix for evaluating classification performance.\n",
    "    \n",
    "    Args:\n",
    "    outputs (torch.Tensor): Model outputs, assumed to be logits.\n",
    "    targets (torch.Tensor): Ground truth labels.\n",
    "    num_classes (int): Number of classes.\n",
    "\n",
    "    Returns:\n",
    "    np.ndarray: A confusion matrix.\n",
    "    \"\"\"\n",
    "    preds = torch.argmax(torch.softmax(outputs, dim=1), dim=1)\n",
    "    conf_matrix = confusion_matrix(targets.cpu().numpy(), preds.cpu().numpy(), labels=list(range(num_classes)))\n",
    "    return conf_matrix\n",
    "\n",
    "# Example of usage\n",
    "conf_matrix = compute_confusion_matrix(outputs, targets, 91)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'Levenshtein'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[0;32mIn [53]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mLevenshtein\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m distance \u001b[38;5;28;01mas\u001b[39;00m levenshtein_distance\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21medit_distance_evaluation\u001b[39m(outputs, targets):\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;124;03m    Compute the edit distance at the state level for evaluating classification performance.\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;124;03m    \u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;124;03m    float: Normalized edit distance, representing the Phone Error Rate (PER).\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'Levenshtein'"
     ]
    }
   ],
   "source": [
    "from Levenshtein import distance as levenshtein_distance\n",
    "\n",
    "def edit_distance_evaluation(outputs, targets):\n",
    "    \"\"\"\n",
    "    Compute the edit distance at the state level for evaluating classification performance.\n",
    "    \n",
    "    Args:\n",
    "    outputs (torch.Tensor): Model outputs, assumed to be logits.\n",
    "    targets (torch.Tensor): Ground truth labels.\n",
    "\n",
    "    Returns:\n",
    "    float: Normalized edit distance, representing the Phone Error Rate (PER).\n",
    "    \"\"\"\n",
    "    predicted_states = torch.argmax(torch.softmax(outputs, dim=1), dim=1)\n",
    "    predicted_sequence = predicted_states.cpu().numpy().tolist()\n",
    "    target_sequence = targets.cpu().numpy().tolist()\n",
    "\n",
    "    # Convert state sequences to string format for edit distance calculation\n",
    "    predicted_str = ''.join(map(str, predicted_sequence))\n",
    "    target_str = ''.join(map(str, target_sequence))\n",
    "\n",
    "    # Calculate edit distance\n",
    "    edit_dist = levenshtein_distance(predicted_str, target_str)\n",
    "    per = edit_dist / len(target_sequence)\n",
    "    return per\n",
    "\n",
    "# Example of usage\n",
    "per = edit_distance_evaluation(outputs, targets)\n",
    "print(f\"Phone Error Rate (PER): {per:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
